name "Azure Databricks Rightsize Compute Instances"
rs_pt_ver 20180301
type "policy"
short_description "Checks for instances that have inefficient utilization for a certain lookback period (in days). \n See the [README](https://github.com/flexera-public/policy_templates/tree/master/cost/azure/databricks/rightsize_compute/) and [docs.flexera.com/flexera/EN/Automation](https://docs.flexera.com/flexera/EN/Automation/AutomationGS.htm) to learn more."
long_description ""
doc_link "https://github.com/flexera-public/policy_templates/tree/master/cost/azure/databricks/rightsize_compute/"
category "Cost"
severity "low"
default_frequency "weekly"
info(
  version: "0.5.1",
  provider: "Azure",
  service: "Databricks",
  policy_set: "Databricks",
  recommendation_type: "Usage Reduction",
  hide_skip_approvals: "true"
)

###############################################################################
# Parameters
###############################################################################

parameter "param_email" do
  type "list"
  category "Policy Settings"
  label "Email Addresses"
  description "A list of email addresses to notify."
  default []
end

parameter "param_azure_endpoint" do
  type "string"
  category "Policy Settings"
  label "Azure Endpoint"
  description "Select the API endpoint to use for Azure. Use default value of management.azure.com unless using Azure China."
  allowed_values "management.azure.com", "management.chinacloudapi.cn"
  default "management.azure.com"
end

parameter "param_subscriptions_allow_or_deny" do
  type "string"
  category "Filters"
  label "Allow/Deny Subscriptions"
  description "Allow or Deny entered Subscriptions. See the README for more details."
  allowed_values "Allow", "Deny"
  default "Allow"
end

parameter "param_subscriptions_list" do
  type "list"
  category "Filters"
  label "Allow/Deny Subscriptions List"
  description "A list of allowed or denied Subscription IDs/names. See the README for more details."
  default []
end

parameter "param_regions_allow_or_deny" do
  type "string"
  category "Filters"
  label "Allow/Deny Regions"
  description "Allow or Deny entered regions. See the README for more details."
  allowed_values "Allow", "Deny"
  default "Allow"
end

parameter "param_regions_list" do
  type "list"
  category "Filters"
  label "Allow/Deny Regions List"
  description "A list of allowed or denied regions. See the README for more details."
  default []
end

parameter "param_databricks_workspace_list" do
  type "list"
  category "Filters"
  label "Databricks Workspace Allowed List"
  description "Allowed Databricks Workspace. If empty, all workspaces will be checked"
  default []
end

parameter "param_databricks_cluster_list" do
  type "list"
  category "Filters"
  label "Databricks Cluster Allowed List"
  description "Allowed Databricks Clusters. Name or Cluster ID can be provided.  If empty, all clusters will be checked"
  default []
end

parameter "param_stats_check_both" do
  type "string"
  category "Statistics"
  label "Idle/Utilized for both CPU/Memory or either"
  description "Set whether an instance should be considered idle and/or underutilized only if both CPU and memory are under the thresholds or if either CPU or memory are under. Note: this parameter is only valid when at least one Memory Utilization threshold and one CPU Utilization threshold is NOT set to -1"
  allowed_values "Both CPU and Memory", "Either CPU or Memory"
  default "Both CPU and Memory"
end

parameter "param_stats_threshold" do
  type "string"
  category "Statistics"
  label "Threshold Statistic"
  description "Statistic to use when determining if an instance is idle/underutilized."
  allowed_values "Minimum", "Average", "Maximum", "p90", "p95", "p99"
  default "p90"
end

parameter "param_stats_interval" do
  type "string"
  category "Statistics"
  label "Statistic Interval"
  description "Interval to use for the time span. The time granularity value should be smaller than the selected time range to be useful, otherwise just one value is returned for the lookback period.  For more details please reference Azure Docs: https://learn.microsoft.com/en-us/azure/azure-monitor/metrics/metrics-aggregation-explained"
  allowed_values "1 minute", "5 minutes", "15 minutes", "30 minutes", "1 hour", "6 hours", "12 hours", "1 day"
  default "15 minutes"
end

parameter "param_stats_lookback" do
  type "number"
  category "Statistics"
  label "Statistic Lookback Period"
  description "How many days back to look at utilization metrics for compute resources. This value cannot be set higher than 90 because Azure does not retain metrics for longer than 90 days."
  min_value 1
  max_value 90
  default 30
end

parameter "param_stats_idle_threshold_cpu_value" do
  type "number"
  category "Statistics"
  label "Idle Instance CPU Threshold (%)"
  description "The CPU threshold at which to consider an instance to be 'idle' and therefore be flagged for termination. Set to -1 to ignore CPU utilization"
  min_value -1
  max_value 100
  default 5
end

parameter "param_stats_idle_threshold_mem_value" do
  type "number"
  category "Statistics"
  label "Idle Instance Memory Threshold (%)"
  description "The Memory threshold at which to consider an instance to be 'idle' and therefore be flagged for termination. Set to -1 to ignore memory utilization"
  min_value -1
  max_value 100
  default 5
end

parameter "param_stats_underutil_threshold_cpu_value" do
  type "number"
  category "Statistics"
  label "Underutilized Instance CPU Threshold (%)"
  description "The CPU threshold at which to consider an instance to be 'underutilized' and therefore be flagged for downsizing. Set to -1 to ignore CPU utilization"
  min_value -1
  max_value 100
  default 40
end

parameter "param_stats_underutil_threshold_mem_value" do
  type "number"
  category "Statistics"
  label "Underutilized Instance Memory Threshold (%)"
  description "The Memory threshold at which to consider an instance to be 'underutilized' and therefore be flagged for downsizing. Set to -1 to ignore memory utilization"
  min_value -1
  max_value 100
  default 40
end

###############################################################################
# Authentication
###############################################################################

# Authenticate with Azure
credentials "auth_azure" do
  schemes "oauth2"
  label "Azure"
  description "Select the Azure Resource Manager Credential from the list."
  tags "provider=azure_rm"
end

credentials "auth_databricks" do
  schemes "oauth2"
  label "Credential for Azure Databricks"
  description "Select the Credential for Azure Databricks from the list."
  tags "provider=databricks"
end

credentials "auth_flexera" do
  schemes "oauth2"
  label "Flexera"
  description "Select Flexera One OAuth2 credentials"
  tags "provider=flexera"
end

###############################################################################
# Pagination
###############################################################################

# Pagination support
pagination "pagination_azure" do
  get_page_marker do
    body_path "nextLink"
  end
  set_page_marker do
    uri true
  end
end

# pagination "pagination_databricks_events" do
#   get_page_marker do
#     body_path jq(response, ".next_page.offset")
#   end
#   set_page_marker do
#     body_field "offset"
#   end
# end

###############################################################################
# Datasources & Scripts
###############################################################################

# Get region-specific Flexera API endpoints
datasource "ds_flexera_api_hosts" do
  run_script $js_flexera_api_hosts, rs_optima_host
end

script "js_flexera_api_hosts", type: "javascript" do
  parameters "rs_optima_host"
  result "result"
  code <<-EOS
  host_table = {
    "api.optima.flexeraeng.com": {
	    api: "api.flexera.com",
      flexera: "api.flexera.com",
      fsm: "api.fsm.flexeraeng.com",
      grs: "grs-front.iam-us-east-1.flexeraeng.com",
      ui: "app.flexera.com",
      tld: "flexera.com"
    },
    "api.optima-eu.flexeraeng.com": {
	    api: "api.flexera.eu",
      flexera: "api.flexera.eu",
      fsm: "api.fsm-eu.flexeraeng.com",
      grs: "grs-front.eu-central-1.iam-eu.flexeraeng.com",
      ui: "app.flexera.eu",
      tld: "flexera.eu"
    },
    "api.optima-apac.flexeraeng.com": {
	    api: "api.flexera.au",
      flexera: "api.flexera.au",
      fsm: "api.fsm-apac.flexeraeng.com",
      grs: "grs-front.ap-southeast-2.iam-apac.flexeraeng.com",
      ui: "app.flexera.au",
      tld: "flexera.au"
    }
  }

  result = host_table[rs_optima_host]
EOS
end

# Get applied policy metadata for use later
datasource "ds_applied_policy" do
  request do
    auth $auth_flexera
    host val($ds_flexera_api_hosts, "flexera")
    path join(["/policy/v1/orgs/", rs_org_id, "/projects/", rs_project_id, "/applied-policies", switch(policy_id, join(["/", policy_id]), "")])
  end
  result do
    encoding "json"
    field "id", jmes_path(response, "id")
    field "name", jmes_path(response, "name")
    field "policyAggregateId", jmes_path(response, "policy_aggregate_id")
  end
end

datasource "ds_billing_centers" do
  request do
    auth $auth_flexera
    host rs_optima_host
    path join(["/analytics/orgs/", rs_org_id, "/billing_centers"])
    query "view", "allocation_table"
    header "Api-Version", "1.0"
    header "User-Agent", "RS Policies"
    ignore_status [403]
  end
  result do
    encoding "json"
    collect jmes_path(response, "[*]") do
      field "href", jmes_path(col_item, "href")
      field "id", jmes_path(col_item, "id")
      field "name", jmes_path(col_item, "name")
      field "parent_id", jmes_path(col_item, "parent_id")
    end
  end
end

# Gather top level billing center IDs for when we pull cost data
datasource "ds_top_level_bcs" do
  run_script $js_top_level_bcs, $ds_billing_centers
end

script "js_top_level_bcs", type: "javascript" do
  parameters "ds_billing_centers"
  result "result"
  code <<-EOS
  filtered_bcs = _.filter(ds_billing_centers, function(bc) {
    return bc['parent_id'] == null || bc['parent_id'] == undefined
  })

  result = _.compact(_.pluck(filtered_bcs, 'id'))
EOS
end

datasource "ds_get_optima_cost_dimensions" do
  request do
    auth $auth_flexera
    host rs_optima_host
    path join(["/bill-analysis/orgs/", rs_org_id, "/costs/dimensions"])
    header "Api-Version", "1.0"
    header "User-Agent", "Flexera Policy Template"
  end
  result do
    encoding "json"
    collect jmes_path(response, "dimensions") do
      field "id", jmes_path(col_item, "id")
      field "name", jmes_path(col_item, "name")
      field "type", jmes_path(col_item, "type")
    end
  end
end

datasource "ds_verified_tags" do
  run_script $js_verified_tags, $ds_get_optima_cost_dimensions
end

script "js_verified_tags", type: "javascript" do
  parameters "ds_get_optima_cost_dimensions"
  result "result"
  code <<-EOS
  result = {'tag_azure_databricks_clusterid':0}
  tag_bool = _.some(ds_get_optima_cost_dimensions, function(item) {
    return item["id"] == "tag_azure_databricks_clusterid"
  })
  result = { tag_azure_databricks_clusterid: tag_bool ? 1 : 0 }
EOS
end

datasource "ds_currency_reference" do
  request do
    host "raw.githubusercontent.com"
    path "/flexera-public/policy_templates/master/data/currency/currency_reference.json"
    header "User-Agent", "RS Policies"
  end
end

datasource "ds_currency_code" do
  request do
    auth $auth_flexera
    host rs_optima_host
    path join(["/bill-analysis/orgs/", rs_org_id, "/settings/currency_code"])
    header "Api-Version", "0.1"
    header "User-Agent", "RS Policies"
    ignore_status [403]
  end
  result do
    encoding "json"
    field "id", jmes_path(response, "id")
    field "value", jmes_path(response, "value")
  end
end

datasource "ds_currency" do
  run_script $js_currency, $ds_currency_reference, $ds_currency_code
end

script "js_currency", type:"javascript" do
  parameters "ds_currency_reference", "ds_currency_code"
  result "result"
  code <<-EOS
  symbol = "$"
  separator = ","

  if (ds_currency_code['value'] != undefined) {
    if (ds_currency_reference[ds_currency_code['value']] != undefined) {
      symbol = ds_currency_reference[ds_currency_code['value']]['symbol']

      if (ds_currency_reference[ds_currency_code['value']]['t_separator'] != undefined) {
        separator = ds_currency_reference[ds_currency_code['value']]['t_separator']
      } else {
        separator = ""
      }
    }
  }

  result = {
    symbol: symbol,
    separator: separator
  }
EOS
end

# Get all subscriptions
datasource "ds_azure_subscriptions" do
  request do
    auth $auth_azure
    pagination $pagination_azure
    host $param_azure_endpoint
    path "/subscriptions"
    query "api-version", "2020-01-01"
    header "User-Agent", "RS Policies"
    # Header X-Meta-Flexera has no affect on datasource query, but is required for Meta Policies
    # Forces `ds_is_deleted` datasource to run first during policy execution
    header "Meta-Flexera", val($ds_is_deleted, "path")
    # Ignore status 400, 403, and 404 which can be returned in certain (legacy) types of Azure Subscriptions
    ignore_status [400, 403, 404]
  end
  result do
    encoding "json"
    collect jmes_path(response, "value[*]") do
      field "subscriptionId", jmes_path(col_item, "subscriptionId")
      field "subscriptionName", jmes_path(col_item, "displayName")
    end
  end
end

datasource "ds_azure_subscriptions_filtered" do
  run_script $js_azure_subscriptions_filtered, $ds_azure_subscriptions, $param_subscriptions_allow_or_deny, $param_subscriptions_list
end

script "js_azure_subscriptions_filtered", type: "javascript" do
  parameters "ds_azure_subscriptions", "param_subscriptions_allow_or_deny", "param_subscriptions_list"
  result "result"
  code <<-EOS
  var result = [];
  if (param_subscriptions_list.length > 0) {
    _.each(ds_azure_subscriptions, function(subscription, idx){
      include_subscription = _.contains(param_subscriptions_list, subscription['subscriptionId']) || _.contains(param_subscriptions_list, subscription['subscriptionName'])

      if (param_subscriptions_allow_or_deny == "Deny") {
        include_subscription = !include_subscription
      }

      if (include_subscription) {
        result.push(subscription)
      }
    })
  } else {
    result = ds_azure_subscriptions;
  }
EOS
end

#get workspaces, and urls
datasource "ds_databricks_workspaces" do
  iterate $ds_azure_subscriptions_filtered
  request do
    auth $auth_azure
    pagination $pagination_azure
    host $param_azure_endpoint
    path join(["/subscriptions/", val(iter_item, "subscriptionId"), "/providers", "/Microsoft.Databricks", "/workspaces"])
    query "api-version", "2023-02-01"
    header "User-Agent", "RS Policies"
    ignore_status [400, 403, 404]
  end
  result do
    encoding "json"
    collect jmes_path(response, "value[*]") do
      field "subscriptionId", jmes_path(iter_item, "subscriptionId")
      field "subscriptionName", jmes_path(iter_item, "subscriptionName")
      field "resourceId", jmes_path(col_item, "id")
      field "name", jmes_path(col_item, "name")
      field "location", jmes_path(col_item, "location")
      field "workspaceId", jmes_path(col_item, "properties.workspaceId")
      field "workspaceUrl", jmes_path(col_item, "properties.workspaceUrl")
      field "managedResourceGroupId", jmes_path(col_item, "properties.managedResourceGroupId")
    end
  end
end

datasource "ds_databricks_workspaces_region_filtered" do
  run_script $js_databricks_workspaces_region_filtered, $ds_databricks_workspaces, $param_regions_allow_or_deny, $param_regions_list
end

script "js_databricks_workspaces_region_filtered", type: "javascript" do
  parameters "ds_databricks_workspaces", "param_regions_allow_or_deny", "param_regions_list"
  result "result"
  code <<-EOS
  var result = [];
  if (param_regions_list.length > 0) {
    result = _.filter(ds_databricks_workspaces, function(item, idx){
      include = _.contains(param_regions_list, item['location'])

      if (param_regions_allow_or_deny == "Deny") {
        include = !include
      }

      return include;
    })
  } else {
    result = ds_databricks_workspaces
  }
EOS
end

datasource "ds_databricks_workspaces_filtered" do
  run_script $js_databricks_workspaces_filtered, $ds_databricks_workspaces_region_filtered, $param_databricks_workspace_list
end

script "js_databricks_workspaces_filtered", type: "javascript" do
  parameters "ds_databricks_workspaces_region_filtered", "param_databricks_workspace_list"
  result "results"
  code <<-EOS
  var results = []
  if ( param_databricks_workspace_list.length != 0 ) {
    _.each(param_databricks_workspace_list, function(workspace){
      var found_workspace = _.find(ds_databricks_workspaces_region_filtered, function(item) {
        return item.workspaceId.toLowerCase() == workspace.toLowerCase() || item.name.toLowerCase() == workspace.toLowerCase()
      });
      if (found_workspace != undefined) {
        results.push(found_workspace)
      }
    });
  } else {
    results = ds_databricks_workspaces_region_filtered
  }
EOS
end

datasource "ds_databricks_workspaces_filtered_indexed" do
  run_script $js_databricks_workspaces_filtered_indexed, $ds_databricks_workspaces_filtered
end

script "js_databricks_workspaces_filtered_indexed", type: "javascript" do
  parameters "ds_databricks_workspaces_filtered"
  result "results"
  code <<-EOS
  var results = _.groupBy(ds_databricks_workspaces_filtered, function(item) {
    return item.managedResourceGroupId.toLowerCase() || null;
  });
EOS
end

datasource "ds_param_stats_interval" do
  run_script $js_param_stats_interval, $param_stats_interval
end

script "js_param_stats_interval", type: "javascript" do
  parameters "param_stats_interval"
  result "result"
  code <<-EOS
  // Tables to convert human-readable parameter values to their API equivalents
  map = {
    "1 minute": "PT1M",
    "5 minutes": "PT5M",
    "15 minutes": "PT15M",
    "30 minutes": "PT30M",
    "1 hour": "PT1H",
    "6 hours": "PT6H",
    "12 hours": "PT12H",
    "1 day": "P1D"
  }

  result = {
    pretty: param_stats_interval,
    api: map[param_stats_interval]
  }
  EOS
end

datasource "ds_lookback_time_formats" do
  run_script $js_lookback_time_formats, $param_stats_lookback
end

script "js_lookback_time_formats", type: "javascript" do
  parameters "param_stats_lookback"
  result "result"
  code <<-EOS
  // Get the "timespan" format which is needed for Azure Metrics API
  end_date = new Date()
  end_date.setMilliseconds(999)
  end_date.setSeconds(59)
  end_date.setMinutes(59)
  end_date.setHours(23)

  start_date = new Date()
  start_date.setDate(start_date.getDate() - param_stats_lookback)
  start_date.setMilliseconds(0)
  start_date.setSeconds(0)
  start_date.setMinutes(0)

  azure_timespan = start_date.toISOString() + "/" + end_date.toISOString()

  // Get the "start_at" and "end_at" format which is needed for Flexera Cost API
  end_date = new Date()
  end_date.setMonth(end_date.getMonth() + 1)
  // Convert to YYYY-MM
  end_date = end_date.toISOString().split("T")[0].split("-").slice(0, 2).join("-")

  start_date = new Date()
  start_date.setDate(start_date.getDate() - param_stats_lookback)
  start_date = start_date.toISOString().split("T")[0].split("-").slice(0, 2).join("-")

  // convert

  flexera_start_at = start_date
  flexera_end_at = end_date

  // Put all the values in an object so values can be used as needed
  result = {
    azure_timespan: azure_timespan,
    flexera_start_at: flexera_start_at,
    flexera_end_at: flexera_end_at
  }
EOS
end

datasource "ds_azure_instance_size_map" do
  request do
    host "raw.githubusercontent.com"
    path "/flexera-public/policy_templates/master/data/azure/instance_types.json"
    header "User-Agent", "RS Policies"
  end
end

datasource "ds_azure_skus" do
  iterate $ds_azure_subscriptions_filtered
  request do
    auth $auth_azure
    pagination $pagination_azure
    host $param_azure_endpoint
    path join(["/subscriptions/", val(iter_item, "subscriptionId"), "/providers/Microsoft.Compute/skus"])
    query "api-version", "2017-09-01"
    header "User-Agent", "RS Policies"
    # Ignore status 400, 403, and 404 which can be returned in certain (legacy) types of Azure Subscriptions
    ignore_status [400, 403, 404]
  end
  result do
    encoding "json"
    collect jmes_path(response, "value") do
      field "resourceType", jmes_path(col_item, "resourceType")
      field "name", jmes_path(col_item, "name")
      field "locations", jmes_path(col_item, "locations")
      field "capabilities", jmes_path(col_item, "capabilities")
      field "restrictions", jmes_path(col_item, "restrictions")
    end
  end
end

datasource "ds_param_stats_threshold" do
  run_script $js_param_stats_threshold, $param_stats_threshold
end

script "js_param_stats_threshold", type: "javascript" do
  parameters "param_stats_threshold"
  result "result"
  code <<-EOS
  switch(param_stats_threshold) {
    case "Minimum":
      result = {
        "param_value" : param_stats_threshold,
        "aggregation" : "minimum"
      }
      break;
    case "Average":
      result = {
        "param_value" : param_stats_threshold,
        "aggregation" : "average"
      }
      break;
    case "Maximum":
      result = {
        "param_value" : param_stats_threshold,
        "aggregation" : "maximum"
      }
      break;
    case "p90":
        result = {
          "param_value" : param_stats_threshold,
          "aggregation" : "p90"
        }
        break;
    case "p95":
      result = {
        "param_value" : param_stats_threshold,
        "aggregation" : "p95"
      }
      break;
    case "p99":
      result = {
        "param_value" : param_stats_threshold,
        "aggregation" : "p99"
      }
      break;
  }
EOS
end

datasource "ds_azure_sku_memory" do
  run_script $js_azure_sku_memory, $ds_azure_skus
end

script "js_azure_sku_memory", type: "javascript" do
  parameters "ds_azure_skus"
  result "result"
  code <<-EOS
  result = {}
  _.each(ds_azure_skus, function (sku, idx){
    name = sku['name'].toLowerCase()

    if (result[name] == undefined && sku["resourceType"] == "virtualMachines") {
      memory = _.find(sku['capabilities'], function(item) {
        return item["name"] == "MemoryGB"
      })

      result[name] = memory["value"] * 1024 * 1024 * 1024
    }
  })
EOS
end

datasource "ds_requests_azure_databricks_managed_vm_metrics" do
  run_script $js_requests_azure_databricks_managed_vm_metrics, $ds_virtualmachines_from_bill, $ds_param_stats_interval, $ds_lookback_time_formats, $param_azure_endpoint
end

script "js_requests_azure_databricks_managed_vm_metrics", type: "javascript" do
  parameters "ds_virtualmachines_from_bill", "ds_param_stats_interval", "ds_lookback_time_formats", "param_azure_endpoint"
  result "requests"
  code <<-EOS
  var requests = [];
  _.each(ds_virtualmachines_from_bill, function(iter_item) {
    // Location and Managed Resource Group ID
    var location = iter_item.databricks_workspace.location || "foo";
    var managedResourceGroupId = iter_item.databricks_workspace.managedResourceGroupId;
    // Get the Subscription from Managed Resource Group ID
    subscription_id = managedResourceGroupId.split("/")[2];

    // Get Resource IDs from Virtual Machines
    var resources = _.pluck(iter_item.virtualmachines, "resourceId");
    // Lowercase the resource ids which is what getBatch API expects
    _.each(resources, function(r, index) {
      resources[index] = resources[index].toLowerCase()
    });

    var request = {
      auth: "auth_azure",
      pagination: "pagination_azure",
      host: location+".metrics.monitor.azure.com",
      verb: "POST",
      path: "/subscriptions/"+subscription_id+"/metrics:getBatch",
      query_params: {
        "api-version": "2023-03-01-preview",
        "timespan": ds_lookback_time_formats["azure_timespan"],
        "interval": ds_param_stats_interval["api"],
        "aggregation": ["minimum", "average", "maximum"].join(","),
        "region": location,
        "metricNamespace": "microsoft.compute/virtualmachines",
        "metricnames": ["Percentage CPU", "Available Memory Bytes"].join(","),
        "orderby": "maximum asc" // Maximum ascending to get the least used VMs first
      },
      headers: {
        "User-Agent": "RS Policies",
        "Content-Type": "application/json"
      },
      body: JSON.stringify({ "resourceids": resources })
    };
    requests.push(request);
  })
EOS
end

datasource "ds_azure_databricks_managed_vm_metrics" do
  iterate $ds_requests_azure_databricks_managed_vm_metrics
  request do
    run_script $js_azure_databricks_managed_vm_metrics, iter_item
  end
  result do
    encoding "json"
    collect jmes_path(response, "values") do
      field "cost", jmes_path(col_item, "cost")
      field "interval", jmes_path(col_item, "interval")
      field "namespace", jmes_path(col_item, "namespace")
      field "resourceid", jmes_path(col_item, "resourceid")
      field "resourceregion", jmes_path(col_item, "resourceregion")
      field "timespan", jmes_path(col_item, "timespan")
      field "value", jmes_path(col_item, "value")
    end
  end
end

script "js_azure_databricks_managed_vm_metrics", type: "javascript" do
  parameters "iter_item"
  result "request"
  code <<-EOS
  request = iter_item
EOS
end

datasource "ds_databricks_clusters" do
  iterate $ds_databricks_workspaces_filtered
  request do
    auth $auth_databricks
    host val(iter_item, "workspaceUrl")
    path "/api/2.0/clusters/list"
    # Ignore 400 gets returned when using an AzureRM credential for the databricks credential ["Expected aud claim to be: 2ff814a6-3304-4ab8-85cb-cd0e6f879c1d, but was: https://management.azure.com"]
    # Also ignore 401/403 if the Service Principal has the correct claim but just doesn't have access to the DB Workspace
    ignore_status [400, 401, 403, 404]
  end
  result do
    encoding "json"
    collect jmes_path(response, "clusters[*]") do
      field "databricks_workspace", iter_item
      field "cluster", col_item
    end
  end
end

datasource "ds_filtered_workspace_virtualmachines" do
  run_script $js_filtered_workspace_virtualmachines, $ds_databricks_workspaces_filtered_indexed, $ds_azure_databricks_managed_vm_metrics, $ds_param_stats_threshold, $ds_databricks_workspaces_vm_costs, $ds_lookback_time_formats, $ds_databricks_clusters, $ds_azure_sku_memory, $param_stats_underutil_threshold_cpu_value
end

script "js_filtered_workspace_virtualmachines", type: "javascript" do
  parameters "ds_databricks_workspaces_filtered_indexed", "ds_azure_databricks_managed_vm_metrics", "ds_param_stats_threshold", "ds_databricks_workspaces_vm_costs", "ds_lookback_time_formats", "ds_databricks_clusters", "ds_azure_sku_memory", "param_stats_underutil_threshold_cpu_value"
  result "result"
  code <<-EOS
  var lastTick = 0; // Holds the timestamp last tick was called
  var tickCount = 0; // Holds the count tick was called

  function percentile(arr, p) {
    // No need to do math if array is empty
    if (arr.length === 0) return 0;
    // array needs to be sorted to select based on percentile
    arr.sort(function (a, b) {
      return a - b;
    });
    // Get the index using desired percentile and array length
    var index = (p / 100) * arr.length;
    // Round index value to nearest integer and return the value of that element in sorted array arr
    return arr[Math.ceil(index) - 1];
  }

  // Index ds_azure_databricks_managed_vm_metrics by VM (resourceid)
  azure_databricks_managed_vm_metrics_indexed = _.groupBy(ds_azure_databricks_managed_vm_metrics, function(item) {
    return item.resourceid.toLowerCase();
  });

  // Placeholder for the result array
  var result = [];

  _.each(_.keys(azure_databricks_managed_vm_metrics_indexed), function(key) {
    vm_resourceId = azure_databricks_managed_vm_metrics_indexed[key][0].resourceid;
    // Get just the resource group id (including /subscriptions/.../resourceGroups/ part) from the resourceId
    vm_resourceGroupId = vm_resourceId.split("/").slice(0, 5).join("/");
    // Get the databricks workspace for this resource
    var databricks_workspace = ds_databricks_workspaces_filtered_indexed[vm_resourceGroupId.toLowerCase()];
    if (databricks_workspace == undefined || databricks_workspace.length === 0) {
      return; // Skip this VM if we can't map it to a workspace
    }
    databricks_workspace = databricks_workspace[0]; // Select the workspace object from the array
    databricks_workspace["resourceGroup"] = databricks_workspace["resourceId"].split("/")[4];

    // Setup placeholder result and calculation values for this resource
    var r = {
      "databricks_workspace": databricks_workspace,
      "resourceId": vm_resourceId,
      "metricsForPeriod": {
        "timespan": ds_lookback_time_formats["azure_timespan"],
        "memory_minimum": null,
        "memory_minimum_p95": null,
        "memory_minimum_p99": null,
        "memory_average": null,
        "memory_average_p95": null,
        "memory_average_p99": null,
        "memory_maximum": null,
        "memory_maximum_p95": null,
        "memory_maximum_p99": null,
        "memory_p95": null,
        "memory_p99": null,
        "cpu_minimum": null,
        "cpu_minimum_p95": null,
        "cpu_minimum_p99": null,
        "cpu_average": null,
        "cpu_average_p95": null,
        "cpu_average_p99": null,
        "cpu_maximum": null,
        "cpu_maximum_p95": null,
        "cpu_maximum_p99": null,
        "cpu_p95": null,
        "cpu_p99": null
      }
    };

    _.each(azure_databricks_managed_vm_metrics_indexed[key], function(vm_metrics) {
      _.each(vm_metrics.value, function(metricNames) {
        metricName = metricNames.name.value
        switch (metricName) {
          case "Percentage CPU":
            metricName = "cpu"
            break;
          case "Available Memory Bytes":
            metricName = "memory"
            break;
          default:
            metricName = "unknown"
        }
        // Placeholder to hold the series for this metric
        _.each(metricNames.timeseries, function(resourceMetrics, index) {
          // Pluck metric values for timespans
          var stats_data_names = ["minimum","average","maximum"] // convenient place to declare the stats we want to loop through
          var all_measurements = []
          _.each(stats_data_names, function(stat) {
            var measurements = _.compact(_.pluck(resourceMetrics.data, stat))
            // Append measurements for this stat to all_measurements
            // We use all measurements (min, avg, max) to calculate percentile
            all_measurements = all_measurements.concat(measurements)
            if (measurements.length > 0) {
              r["metricsForPeriod"][metricName+"_"+stat+"_measurements"] = measurements
              switch(stat) {
                case "minimum":
                  r["metricsForPeriod"][metricName+"_minimum"] = _.min(measurements)
                  break;
                case "average":
                  r["metricsForPeriod"][metricName+"_average"] = _.reduce(measurements, function(a, b){ return a + b; }, 0) / measurements.length
                  break;
                case "maximum":
                  r["metricsForPeriod"][metricName+"_maximum"] = _.max(measurements)
                  break;
              }
            }
            // We also need to store the timestamp series for the imagechart later
            var series = [];
            for (i = 0; i < resourceMetrics.data.length; i++) {
              series.push({
                "timeStamp": resourceMetrics.data[i].timeStamp,
                "value": resourceMetrics.data[i][stat] || "_" // Default to "_" if no value for imagecharts
              });
            }
            r["metricsForPeriod"][metricName+"_"+stat+"_series"] = series
          })
          r["metricsForPeriod"][metricName+"_p90"] = percentile(all_measurements, 90)
          r["metricsForPeriod"][metricName+"_p95"] = percentile(all_measurements, 95)
          r["metricsForPeriod"][metricName+"_p99"] = percentile(all_measurements, 99)
          r["metricsForPeriod"][metricName+"_measurement_count"] = all_measurements.length
        })
      })
    });

    // Add the VM to the result array
    result.push(r)
  });
  EOS
end

datasource "ds_filtered_workspace_virtualmachines_with_costs" do
  run_script $js_filtered_workspace_virtualmachines_with_costs, $ds_filtered_workspace_virtualmachines, $ds_databricks_workspaces_vm_costs, $ds_databricks_clusters, $ds_azure_sku_memory, $param_stats_lookback
end

script "js_filtered_workspace_virtualmachines_with_costs", type: "javascript" do
  parameters "ds_filtered_workspace_virtualmachines", "ds_databricks_workspaces_vm_costs", "ds_databricks_clusters", "ds_azure_sku_memory", "param_stats_lookback"
  result "result"
  code <<-EOS
  // Index ds_databricks_workspaces_vm_costs for quicker lookup
  ds_databricks_workspaces_vm_costs_indexed = _.groupBy(ds_databricks_workspaces_vm_costs, function(item) {
    return item["resourceId"].toLowerCase();
  });
  // Index ds_databricks_clusters for quicker lookup
  ds_databricks_clusters_indexed = _.groupBy(ds_databricks_clusters, function(item) {
    return item["databricks_workspace"]["resourceId"].toLowerCase();
  });
  var result = [];
  _.each(ds_filtered_workspace_virtualmachines, function(r, idx) {
    // At this point we have all the utilization data from Azure Metrics API
    // We need to look at the data from Flexera to get cost, databricks cluster id, and cluster instance type

    // Filters costs for this resources from the datasource
    resource_costs = ds_databricks_workspaces_vm_costs_indexed[r["resourceId"].toLowerCase()] || [];

    // Check if we found cost information for this resource
    if (resource_costs.length > 0) {
      // If we did, add it to the result -- starting at 0
      r["total_cost"] = 0
      // Loop through the costs and add them up
      _.each(resource_costs, function(cost) {
        r["total_cost"] = r["total_cost"] + cost["cost"]
        // Prevent divide by zero  / +Inf type errors
        if (cost["usage_amount"] > 0) {
          r["unit_cost"] = cost["cost"] / cost["usage_amount"]
        }
        r["databricks_cluster_id"] = cost["cluster_id"]
        r["resourceType"] = cost["resourceType"]
      })
      // Handle case where param_stats_lookback is not 30 days (1month)
      // The Optimization dashboard reports savings in $/month and so we need to match that unit regardless of the lookback period
      if (param_stats_lookback != 30) {
        // If we're not looking back 30 days, we need to calculate how much of a "month" (30days) we are looking at
        month_fraction = param_stats_lookback / 30
        // Now we can take the total cost for the period, and the portion of the month to estimate the total cost for the month
        r["monthly_cost_estimate"] = r["total_cost"] / month_fraction
      } else {
        // If we are looking back 30 days, we will use the total_cost as the estimated savings
        r["monthly_cost_estimate"] = r["total_cost"]
      }
      // Add lookbackPeriod for the resource since we have this value in memory already
      r["lookbackPeriod"] = param_stats_lookback
      // Now that we know the instance type, we can get the total memory from ds_azure_sku_memory
      r["metricsForPeriod"]["memory_total"] = ds_azure_sku_memory[r["resourceType"].toLowerCase()]
    }

    // With memory_total, we can now calculate percent memory
    if (typeof r["metricsForPeriod"]["memory_total"] == "number" && r["metricsForPeriod"]["memory_total"] > 0) {
      if (typeof r["metricsForPeriod"]["memory_minimum"] == "number" && r["metricsForPeriod"]["memory_minimum"] > 0) {
        r["metricsForPeriod"]["memory_minimum_percent"] = (r["metricsForPeriod"]["memory_minimum"] / r["metricsForPeriod"]["memory_total"]) * 100
      }
      if (typeof r["metricsForPeriod"]["memory_average"] == "number" && r["metricsForPeriod"]["memory_average"] > 0) {
        r["metricsForPeriod"]["memory_average_percent"] = (r["metricsForPeriod"]["memory_average"] / r["metricsForPeriod"]["memory_total"]) * 100
      }
      if (typeof r["metricsForPeriod"]["memory_maximum"] == "number" && r["metricsForPeriod"]["memory_maximum"] > 0) {
        r["metricsForPeriod"]["memory_maximum_percent"] = (r["metricsForPeriod"]["memory_maximum"] / r["metricsForPeriod"]["memory_total"]) * 100
      }
      if (typeof r["metricsForPeriod"]["memory_p90"] == "number" && r["metricsForPeriod"]["memory_p90"] > 0) {
        r["metricsForPeriod"]["memory_p90_percent"] = (r["metricsForPeriod"]["memory_p90"] / r["metricsForPeriod"]["memory_total"]) * 100
      }
      if (typeof r["metricsForPeriod"]["memory_p95"] == "number" && r["metricsForPeriod"]["memory_p95"] > 0) {
        r["metricsForPeriod"]["memory_p95_percent"] = (r["metricsForPeriod"]["memory_p95"] / r["metricsForPeriod"]["memory_total"]) * 100
      }
      if (typeof r["metricsForPeriod"]["memory_p99"] == "number" && r["metricsForPeriod"]["memory_p99"] > 0) {
        r["metricsForPeriod"]["memory_p99_percent"] = (r["metricsForPeriod"]["memory_p99"] / r["metricsForPeriod"]["memory_total"]) * 100
      }
    }
    if (typeof r["databricks_cluster_id"] === "string") {
      // Attempt to find additional cluster metadata in ds_databricks_clusters
      r["databricks_cluster"] = ds_databricks_clusters_indexed[r["databricks_cluster_id"].toLowerCase()];
    }

    // For each of our metricsForPeriod, lets round to 2 decimals
    _.each(_.keys(r["metricsForPeriod"]), function(metric) {
      if (typeof r["metricsForPeriod"][metric] != "null" && typeof r["metricsForPeriod"][metric] != "undefined" && r["metricsForPeriod"][metric] > 0) {
        r["metricsForPeriod"][metric] = Math.round(r["metricsForPeriod"][metric] * 100) / 100
      }
    })

    // Rounding
    // Total Cost to 2 digits
    r["total_cost"] = Math.round(r["total_cost"] * 100) / 100
    // Estimated Monthly Cost to 2 digits
    r["monthly_cost_estimate"] = Math.round(r["monthly_cost_estimate"] * 100) / 100
    // Unit Cost to 4 digits (portions of a penny because that's how price is publicly listed)
    r["unit_cost"] = Math.round(r["unit_cost"] * 10000) / 10000

    result.push(r)
  })
  EOS
end

datasource "ds_databricks_workspaces_vm_costs" do
  iterate $ds_databricks_workspaces_filtered
  request do
    run_script $js_databricks_workspaces_vm_costs, iter_item, $ds_top_level_bcs, $ds_lookback_time_formats, $param_databricks_cluster_list, rs_org_id, rs_optima_host
  end
  result do
    encoding "json"
    collect jmes_path(response, "rows[*]") do
      field "subscriptionId", jmes_path(iter_item, "subscriptionId")
      field "subscriptionName", jmes_path(iter_item, "subscriptionName")
      field "resourceId", jmes_path(col_item, "dimensions.resource_id")
      field "cluster_id", jmes_path(col_item, "dimensions.tag_azure_databricks_clusterid")
      field "usage_unit", jmes_path(col_item, "dimensions.usage_unit")
      field "resourceType", jmes_path(col_item, "dimensions.instance_type")
      field "region", jmes_path(col_item, "dimensions.region")
      field "resource_group", jmes_path(col_item, "dimensions.resource_group")
      field "vendor_account", jmes_path(col_item, "dimensions.vendor_account")
      field "cost", jmes_path(col_item, "metrics.cost_amortized_unblended_adj")
      field "usage_amount", jmes_path(col_item, "metrics.usage_amount")
    end
  end
end

script "js_databricks_workspaces_vm_costs", type: "javascript" do
  parameters "iter_item", "ds_top_level_bcs", "ds_lookback_time_formats", "param_databricks_cluster_list", "rs_org_id", "rs_optima_host"
  result "request"
  code <<-EOS
  subscription_id = iter_item["managedResourceGroupId"].split("/")[2]
  // Select the resource group name from the managedResourceGroupId and lowercase to match what's provided in Azure Bill Data (RGs are always lowercase)
  resource_group = iter_item["managedResourceGroupId"].split("/")[4].toLowerCase()

  var request = {
    auth: "auth_flexera",
    host: rs_optima_host,
    verb: "POST",
    path: "/bill-analysis/orgs/" + rs_org_id + "/costs/select",
    body_fields: {
      dimensions: ["resource_id","tag_azure_databricks_clusterid","usage_unit","instance_type","vendor_account","region","resource_group"],
      granularity: "month",
      start_at: ds_lookback_time_formats["flexera_start_at"],
      end_at: ds_lookback_time_formats["flexera_end_at"],
      metrics: ["cost_amortized_unblended_adj","usage_amount"],
      billing_center_ids: ds_top_level_bcs,
      limit: 100000,
      filter: {
        "type": "and",
        "expressions": [
          // Microsoft.Compute Service
          {
            "type": "or",
            "expressions": [
              {
                "dimension": "service",
                "type": "equal",
                "value": "Microsoft.Compute"
              },
              {
                "dimension": "service",
                "type": "equal",
                "value": "microsoft.compute"
              }
            ]
          },
          // Virtual Machine Resources
          {
            "dimension": "resource_type",
            "type": "substring",
            "substring": "Virtual Machines-"
          },
          // Vendor Account + Resource Group Filters
          {
            "dimension": "vendor_account",
            "type": "equal",
            "value": subscription_id
          },
          {
            "dimension": "resource_group",
            "type": "equal",
            "value": resource_group
          },
          // Exclude Shared Cost adjustments (if exist)
          {
            "type": "not",
            "expression": {
              "dimension": "adjustment_name",
              "type": "substring",
              "substring": "Shared"
            }
          }
        ]
      }
    },
    headers: {
      'User-Agent': "RS Policies",
      'Api-Version': "1.0"
    }
  }
  // Filter the Resulting VMs to only VMs in the clusters defined in the policy param
  if (param_databricks_cluster_list.length > 0) {
    cluster_filter = {
      "type": "or",
      "expressions": [
      ]
    }
    _.each(param_databricks_cluster_list, function(cluster) {
      cluster_filter["expressions"].push({
        "dimension": "tag_azure_databricks_clusterid",
        "type": "equal",
        "value": cluster
      })
    })
    request.body_fields.filter.expressions.push(cluster_filter)
  }
EOS
end

datasource "ds_virtualmachines_from_bill" do
  run_script $js_virtualmachines_from_bill, $ds_databricks_workspaces_vm_costs, $ds_databricks_workspaces_filtered_indexed
end

script "js_virtualmachines_from_bill", type: "javascript" do
  parameters "ds_databricks_workspaces_vm_costs", "ds_databricks_workspaces_filtered_indexed"
  result "result"
  code <<-EOS
  var result = [];
  var result_map = {}
  _.each(ds_databricks_workspaces_vm_costs, function(vm, idx) {
    vm_cost_sub = vm["resourceId"].split("/")[2] // Get the subscription ID from the resource ID
    vm_cost_rg = vm["resourceId"].split("/")[4] // Get the resource group from the resource ID
    vm_rg_string = "/subscriptions/"+vm_cost_sub+"/resourceGroups/"+vm_cost_rg

    var db_workspace = ds_databricks_workspaces_filtered_indexed[vm_rg_string.toLowerCase()][0];
    if (db_workspace != undefined) {
      if (result_map[vm_cost_sub] == undefined || result_map[vm_cost_sub] == null) {
        result_map[vm_cost_sub] = {}
      }
      if (result_map[vm_cost_sub][db_workspace.location] == undefined || result_map[vm_cost_sub][db_workspace.location] == null) {
        result_map[vm_cost_sub][db_workspace.location] = {
          "databricks_workspace": db_workspace,
          "virtualmachines": []
        }
      }
      result_map[vm_cost_sub][db_workspace.location]["virtualmachines"].push(vm)
    }
  })

  // The policy evaluation expects an array, and we don't need the keys anymore so lets convert it to an array
  _.each(Object.keys(result_map), function(subscriptionId) {
    _.each(Object.keys(result_map[subscriptionId]), function(location) {
      vm_count = result_map[subscriptionId][location]["virtualmachines"].length

      // The getBatch metrics API expects groups of <=50 resources, so we need to chunk the results
      // Using 25 as the chunk size because batch responses with PT1M can be > 8MB which is policy limit
      chunk_size = 25
      if (vm_count > 0 && vm_count <= chunk_size) {
        // If less <= chunk_size, just add the result as it is
        result.push(result_map[subscriptionId][location])
      } else {
        // If more than chunk_size, do the chunking
        for (var i=0; i < vm_count; i+=chunk_size) {
          result.push({
            // Add metadata to datasource results that this workspace has been chunked
            // Helpful when debugging/troubleshooting
            "chunkedWorkspaceIntoMultiple": true,
            "chunkIndex": i,
            "databricks_workspace": result_map[subscriptionId][location]["databricks_workspace"],
            "virtualmachines": result_map[subscriptionId][location]["virtualmachines"].slice(i, i + chunk_size)
          })
        }
      }
    })
  })
  EOS
end

datasource "ds_idle_and_underutil_instances" do
  run_script $js_idle_and_underutil_instances, $ds_param_stats_threshold, $ds_filtered_workspace_virtualmachines_with_costs, $ds_azure_instance_size_map, $ds_currency, $ds_applied_policy, $ds_flexera_api_hosts, $ds_param_stats_interval, $param_stats_check_both, $param_stats_idle_threshold_cpu_value, $param_stats_underutil_threshold_cpu_value, $param_stats_idle_threshold_mem_value, $param_stats_underutil_threshold_mem_value, $param_stats_lookback, rs_org_id, rs_project_id
end

script "js_idle_and_underutil_instances", type: "javascript" do
  parameters "ds_param_stats_threshold", "ds_filtered_workspace_virtualmachines_with_costs", "ds_azure_instance_size_map", "ds_currency", "ds_applied_policy", "ds_flexera_api_hosts", "ds_param_stats_interval", "param_stats_check_both", "param_stats_idle_threshold_cpu_value", "param_stats_underutil_threshold_cpu_value", "param_stats_idle_threshold_mem_value", "param_stats_underutil_threshold_mem_value", "param_stats_lookback", "rs_org_id", "rs_project_id"
  result "result"
  code <<-EOS
  // Used for formatting numbers to look pretty
  function formatNumber(number, separator) {
    formatted_number = "0"

    if (number) {
      formatted_number = (Math.round(number * 100) / 100).toString().split(".")[0]

      if (separator) {
        withSeparator = ""

        for (var i = 0; i < formatted_number.length; i++) {
          if (i > 0 && (formatted_number.length - i) % 3 == 0) { withSeparator += separator }
          withSeparator += formatted_number[i]
        }

        formatted_number = withSeparator
      }

      decimal = (Math.round(number * 100) / 100).toString().split(".")[1]
      if (decimal) { formatted_number += "." + decimal }
    }

    return formatted_number
  }

  // Function to generate ImageCharts URL for CPU and Memory usage
  function generateChartUrl(metricsForPeriod, stat_name, resource_name) {

    // Check that we have some metrics for either cpu or memory
    if ((!metricsForPeriod || _.keys(metricsForPeriod).length === 0)) {
      // return null if we don't have any data
      return null;
    }

    // Force stat_name to lowercase
    stat_name = stat_name.toLowerCase()
    // Handle p90, p95, and p99 which are not available from Azure
    if ((stat_name == "p90") || (stat_name == "p95") || (stat_name == "p99")) {
      // Azure does not provide these aggregations and we can't calculate them due to policy limitations
      // Use maximum instead when policy configured for p90, p95, and p99
      stat_name = "maximum"
    }

    // Placeholder for extracted timestamps, CPU and memory values
    var timestampsAll = [];
    var cpuValuesAll = [];
    _.each(metricsForPeriod["cpu_"+stat_name+"_series"], function(item) {
      // Use "_" as null
      timestampsAll.push(item.timeStamp);
      cpuValuesAll.push(item.value || "_");
    });
    var memMeasurements = metricsForPeriod["memory_" + stat_name + "_measurements"] || ["_"];
    // Calculate memory percentage from the measurement of available memory bytes
    var memValuesAll = _.map(memMeasurements, function(value) {
      // If value is not a number, return "_"
      if (typeof value !== "number" || isNaN(value) || !metricsForPeriod || isNaN(metricsForPeriod["memory_total"]) || metricsForPeriod["memory_total"] <= 0) {
        return "_";
      }
      // Calculate memory percentage based on total memory
      return ((value / metricsForPeriod["memory_total"]) * 100).toFixed(2);
    });

    // Check if we have any data points to plot
    if (timestampsAll.length === 0) {
      return null;
    }

    // Limit number of data points to avoid URL length issues
    var timestamps = [];
    var cpuValues = [];
    var memValues = [];
    var maxPoints = 60; // Maximum number of points to plot.  Limited by number of series, and URL length limits
    var step = Math.max(1, Math.floor(timestampsAll.length / maxPoints)); // Using cpu to determine max points and all the timestamps we have data for
    // Loop through the data points, taking every nth point based on calculated step size
    for (var i = 0; i < timestampsAll.length; i += step) {
      // Add timestamp and CPU value for this point
      timestamps.push(timestampsAll[i]);
      cpuValues.push(cpuValuesAll[i]); // Use "_" if no value
      // Add memory value for this point
      memValues.push(memValuesAll[i]); // Use "_" if no value
    }

    // Encode data for URL
    var timeLabels = timestamps.join('|');
    var cpuData = cpuValues.join(',');
    var memData = memValues.join(',');

    var lineColors = [
      "206BB6", // Blue 55
      "CF8F36", // Gold 70
    ]

    // Proper case stat_name
    var first_char = stat_name.charAt(0);
    var rest = stat_name.substring(1);
    var stat_name_Proper = first_char.toUpperCase() + rest.toLowerCase();

    // Build ImageCharts URL - creating a line chart with CPU (blue) and Memory (gold)
    var chartUrl = "https://api.image-charts-auth.flexeraeng.com/ic-function?rs_org_id="+rs_org_id+"&rs_project_id="+rs_project_id;
    chartUrl += "&cht=lc"; // Line chart
    chartUrl += "&chs=900x450"; // Chart size
    chartUrl += "&chco=" + lineColors.join(','); // Colors
    chartUrl += "&chxt=x,y"; // Show both axes
    chartUrl += "&chxs=0,000000,12,-1,lt,000000,s,min90"; // Rotate labels 90 and add padding
    chartUrl += "&chdl="+encodeURIComponent("CPU Usage%25%20%20%20%20%20%20%20%20%20%20|Memory Usage%25"); // Legend
    chartUrl += "&chdlp=b"; // Legend position (bottom)
    chartUrl += "&chtt="+encodeURIComponent(stat_name_Proper+"+Resource+Utilization|" + encodeURIComponent(resource_name)); // Title
    chartUrl += "&chma=10,10,10,10"; // Margins all around
    chartUrl += "&chxr=1,0,100"; // Y-axis range
    chartUrl += "&chls="+encodeURIComponent("4|4"); // Line thickness
    chartUrl += "&chf=bg,s,FFFFFF00"; // Transparent background
    chartUrl += "&chg="+encodeURIComponent("20,50,5,5,CECECE"); // Dashed, grey gridlines
    chartUrl += "&chxl="+encodeURIComponent("0:|" + encodeURIComponent(timeLabels)); // X-axis labels
    chartUrl += "&chd="+encodeURIComponent("t:" + cpuData + "|" + memData); // Chart data

    return chartUrl;
  }

  underutil_total_savings = 0.0
  idle_total_savings = 0.0

  underutil_list = []
  idle_list = []

  applied_policy_name = ds_applied_policy["name"] || "Azure Databricks Rightsize Compute Instances"

  // Determine whether we're checking for CPU, memory, or both
  checking_cpu = param_stats_underutil_threshold_cpu_value != -1 || param_stats_idle_threshold_cpu_value != -1
  checking_mem = param_stats_underutil_threshold_mem_value != -1 || param_stats_idle_threshold_mem_value != -1

  _.each(ds_filtered_workspace_virtualmachines_with_costs, function(vm, idx) {
    // Set the Applied Policy Name which is used in the summary_template/detail_template output
    vm["policy_name"] = applied_policy_name
    // Get the utilization metrics that we are going to use for comparison
    cpu_value = vm.metricsForPeriod["cpu_"+ds_param_stats_threshold["aggregation"]]
    mem_value = vm.metricsForPeriod["memory_"+ds_param_stats_threshold["aggregation"]+"_percent"]

    // Set thresholdType for output
    vm["thresholdType"] = ds_param_stats_threshold["param_value"]
    vm["thresholdResourceTypes"] = param_stats_check_both
    vm["lookbackPeriod"] = param_stats_lookback

    // Set fields for scrapper recommendations service
    vm["savingsCurrency"] = ds_currency['symbol']
    vm["service"] = "Microsoft.Compute"

    // Test for whether to consider the instance idle or underutilized.
    // Assume instance is not idle or underutilized by default.
    is_idle = false
    is_underutil = false

    // Determine if the instance is idle or underutilized for each category.
    // Store boolean result for later use.
    is_idle_cpu = cpu_value < param_stats_idle_threshold_cpu_value
    is_underutil_cpu = cpu_value < param_stats_underutil_threshold_cpu_value
    is_idle_mem = mem_value < param_stats_idle_threshold_mem_value
    is_underutil_mem = mem_value < param_stats_underutil_threshold_mem_value

    // If we're only checking CPU, simply set is_idle/is_underutil to their CPU equivalents
    if (!checking_mem) { is_idle = is_idle_cpu }
    if (!checking_mem) { is_underutil = is_underutil_cpu }

    // If we're only checking memory, simply set is_idle/is_underutil to their memory equivalents
    if (!checking_cpu) { is_idle = is_idle_mem }
    if (!checking_cpu) { is_underutil = is_underutil_mem }

    // If we're checking both, do an 'and' or an 'or' depending on the value of param_stats_check_both
    if (checking_cpu && checking_mem) {
      if (param_stats_check_both == "Both CPU and Memory") {
        is_idle = is_idle_cpu && is_idle_mem
        is_underutil = is_underutil_cpu && is_underutil_mem
      } else {
        is_idle = is_idle_cpu || is_idle_mem
        is_underutil = is_underutil_cpu || is_underutil_mem
      }
    }

    // Only bother doing anything if we're checking at least one metric
    if (checking_cpu || checking_mem) {
      if (is_idle) { // First check to see if the resource is below the lower "idle" thresholds
        vm["recommendationType"] = "Delete"

        recommendationDetails = [
          "Delete Databricks Cluster ", vm["databricks_cluster_id"], " ",
          "which created Azure virtual machine ", vm["resourceId"], " ",
          "in Azure Subscription ", vm["databricks_workspace"]["subscriptionName"], " ",
          "(", vm["databricks_workspace"]["subscriptionId"], ")"
        ]

        // Generate chart URL for this resource
        var stat_name = ds_param_stats_threshold["aggregation"]
        if (stat_name == "p90" || stat_name == "p95" || stat_name == "p99") {
          // If the stat_name is p90, p95, or p99, we need to use maximum instead
          stat_name = "maximum"
        }
        var chartUrl = generateChartUrl(vm.metricsForPeriod, stat_name, vm["resourceId"]);

        // Append utilization chart to recommendationDetails if we have one
        if (_.isString(chartUrl)) {
          // Construct chartUrlField which is in the "link-external" format -- display name||URL
          vm["chartUrlField"] = vm["databricks_cluster_id"]+" Utilization Chart||" + chartUrl

          // Append the chart as image markdown to the recommendation details
          recommendationDetails.push("\\n\\n![Utilization Chart](" + chartUrl + "&from_pt=true)")
        }

        vm["recommendationDetails"] = recommendationDetails.join('')

        if (typeof vm["monthly_cost_estimate"] == "number") {
          vm["savings"] = vm["monthly_cost_estimate"] // Potential Savings from delete is the estimated monthly cost of the resource
          idle_total_savings += vm["monthly_cost_estimate"]
        }
        vm["memoryThreshold"] = param_stats_idle_threshold_mem_value
        vm["cpuThreshold"] = param_stats_idle_threshold_cpu_value
        idle_list.push(vm)
      } else if (is_underutil) { // If not idle, check see if the resource is below the lower "idle" threshold
        // Get the downsize instance type
        if (ds_azure_instance_size_map[vm['resourceType']]) {
          vm["newResourceType"] = ds_azure_instance_size_map[vm['resourceType']]['down']
        }
        vm["recommendationType"] = "Downsize"

        recommendationDetails = [
          "Resize Databricks Cluster ", vm["databricks_cluster_id"], " ",
          "from ", vm["resourceType"], " ",
          "to ", vm["newResourceType"]
        ]

        // Generate chart URL for this resource
        var stat_name = ds_param_stats_threshold["aggregation"]
        if (stat_name == "p90" || stat_name == "p95" || stat_name == "p99") {
          // If the stat_name is p90, p95, or p99, we need to use maximum instead
          stat_name = "maximum"
        }
        var chartUrl = generateChartUrl(vm.metricsForPeriod, stat_name, vm["resourceId"]);

        // Append utilization chart to recommendationDetails if we have one
        if (_.isString(chartUrl)) {
          // Construct chartUrlField which is in the "link-external" format -- display name||URL
          vm["chartUrlField"] = vm["databricks_cluster_id"]+" Utilization Chart||" + chartUrl + "&from_pt=true"

          // Append the chart as image markdown to the recommendation details
          recommendationDetails.push("\\n\\n![Utilization Chart](" + chartUrl + "&from_pt=true)")
        }

        vm["recommendationDetails"] = recommendationDetails.join('')

        if (typeof vm["monthly_cost_estimate"] == "number") {
          vm["savings"] = vm["monthly_cost_estimate"] / 2 // Potential Savings from downsize is generally half the estimated monthly cost of current resource type
          underutil_total_savings += vm["savings"]
        }
        vm["memoryThreshold"] = param_stats_underutil_threshold_mem_value
        vm["cpuThreshold"] = param_stats_underutil_threshold_cpu_value

        // To reduce the incident size, we can drop any metricsForPeriod fields that are not needed in the final incident
        // These fields picked are the ones declared down below in the export fields of the incident
        metricsForPeriod = _.pick(vm.metricsForPeriod, [
          "cpu_minimum", "cpu_average", "cpu_maximum", "cpu_p90", "cpu_p95", "cpu_p99", "cpu_measurement_count",
          "memory_minimum_percent", "memory_average_percent", "memory_maximum_percent", "memory_p90_percent", "memory_p95_percent", "memory_p99_percent", "memory_measurement_count", "memory_total"
        ])
        // Overwrite the metricsForPeriod with the reduced set
        vm["metricsForPeriod"] = metricsForPeriod

        underutil_list.push(vm)
      }
    }
  })

  // Build out the detail_template for the incidents
  if (checking_cpu || checking_mem) {
    instances_total = ds_filtered_workspace_virtualmachines_with_costs.length.toString()
    underutil_instances_total = underutil_list.length.toString()
    underutil_instances_percentage = (underutil_instances_total / instances_total * 100).toFixed(2).toString() + '%'
    idle_instances_total = idle_list.length.toString()
    idle_instances_percentage = (idle_instances_total / instances_total * 100).toFixed(2).toString() + '%'

    underutil_total_savings_formatted = ds_currency['symbol'] + ' ' + formatNumber(parseFloat(underutil_total_savings).toFixed(2), ds_currency['separator'])
    idle_total_savings_formatted = ds_currency['symbol'] + ' ' + formatNumber(parseFloat(idle_total_savings).toFixed(2), ds_currency['separator'])

    underutil_verb = "are"
    if (underutil_instances_total == 1) { underutil_verb = "is" }

    idle_verb = "are"
    if (idle_instances_total == 1) { idle_verb = "is" }

    underutil_findings = [
      "Out of ", instances_total, " Azure Databricks virtual machines analyzed, ",
      underutil_instances_total, " (", underutil_instances_percentage,
      ") ", underutil_verb, " underutilized and recommended for downsizing. "
    ].join('')

    idle_findings = [
      "Out of ", instances_total, " Azure Databricks virtual machines analyzed, ",
      idle_instances_total, " (", idle_instances_percentage,
      ") ", idle_verb, " idle and recommended for termination. "
    ].join('')

    if (checking_cpu && checking_mem) {
      message_boolean = "or"

      if (param_stats_check_both == "Both CPU and Memory") {
        message_boolean = "and"
      }

      underutil_analysis_message = [
        "A virtual machine is considered underutilized if its CPU usage (",
        ds_param_stats_threshold["param_value"].toLowerCase(), ") is below ",
        param_stats_underutil_threshold_cpu_value, "% ", message_boolean,
        " its memory usage (", ds_param_stats_threshold["param_value"].toLowerCase(),
        ") is below ", param_stats_underutil_threshold_mem_value,
        "% but its CPU usage is still above or equal to ",
        param_stats_idle_threshold_cpu_value, "% ", message_boolean,
        " its memory usage is still above or equal to ",
        param_stats_idle_threshold_mem_value, "%. "
      ].join('')

      idle_analysis_message = [
        "A virtual machine is considered idle if its CPU usage (",
        ds_param_stats_threshold["param_value"].toLowerCase(), ") is below ",
        param_stats_idle_threshold_cpu_value, "% ", message_boolean,
        " its memory usage (", ds_param_stats_threshold["param_value"].toLowerCase(),
        ") is below ", param_stats_idle_threshold_mem_value, "%. "
      ].join('')

      lookback_message = [
        "CPU and memory usage was analyzed over the last ",
        param_stats_lookback.toString(), " days. ",
        "These metrics were gathered at a granularity of '",
        ds_param_stats_interval['pretty'], "'.\\n\\n"
      ].join('')
    }

    if (checking_cpu && !checking_mem) {
      underutil_analysis_message = [
        "A virtual machine is considered underutilized if its CPU usage (",
        ds_param_stats_threshold["param_value"].toLowerCase(), ") is below ",
        param_stats_underutil_threshold_cpu_value,
        "% but its CPU usage is still above or equal to ",
        param_stats_idle_threshold_cpu_value, "%. "
      ].join('')

      idle_analysis_message = [
        "A virtual machine is considered idle if its CPU usage (",
        ds_param_stats_threshold["param_value"].toLowerCase(), ") is below ",
        param_stats_idle_threshold_cpu_value, "%. "
      ].join('')

      lookback_message = [
        "CPU usage was analyzed over the last ",
        param_stats_lookback.toString() + " days. ",
        "CPU metrics were gathered at a granularity of '",
        ds_param_stats_interval['pretty'], "'.\\n\\n"
      ].join('')
    }

    if (!checking_cpu && checking_mem) {
      underutil_analysis_message = [
        "A virtual machine is considered underutilized if its memory usage (",
        ds_param_stats_threshold["param_value"].toLowerCase(), ") is below ",
        param_stats_underutil_threshold_mem_value,
        "% but its memory usage is still above or equal to ",
        param_stats_idle_threshold_mem_value, "%. "
      ].join('')

      idle_analysis_message = [
        "A virtual machine is considered idle if its memory usage (",
        ds_param_stats_threshold["param_value"].toLowerCase(), ") is below ",
        param_stats_idle_threshold_mem_value, "%. "
      ].join('')

      lookback_message = [
        "Memory usage was analyzed over the last ",
        param_stats_lookback.toString() + " days. ",
        "Memory metrics were gathered at a granularity of '",
        ds_param_stats_interval['pretty'], "'.\\n\\n"
      ].join('')
    }

    // Generate applied policy URL
    var applied_policy_url = "https://" + ds_flexera_api_hosts["ui"]
    // determine if policyAggregate or regular applied policy
    if (_.isString(ds_applied_policy['policyAggregateId'])) {
      applied_policy_url = applied_policy_url + "/orgs/" + rs_org_id + "/automation/applied-policies?policyId=" + ds_applied_policy['policyAggregateId'];
    } else {
      applied_policy_url = applied_policy_url + "/orgs/" + rs_org_id + "/automation/applied-policies/projects/" + rs_project_id + "?policyId=" + ds_applied_policy['id'];
    }

    disclaimer = "The above settings can be modified by editing the [" + applied_policy_name + "](" + applied_policy_url + ") applied policy and changing the appropriate parameters."

    underutil_message = underutil_findings + underutil_analysis_message + lookback_message + disclaimer
    idle_message = idle_findings + idle_analysis_message + lookback_message + disclaimer
  } else {
    underutil_message = "No results were found because all CPU and memory parameters were set to -1 when this policy was applied. Please terminate and reapply this policy with one of these settings enabled."
    idle_message = underutil_message
  }

  // Sort by descending order of savings value
  idle_list = _.sortBy(idle_list, function(item) { return item['savings'] * -1 })
  underutil_list = _.sortBy(underutil_list, function(item) { return item['savings'] * -1 })

  // Add these to both lists to ensure the first item that fails validation for each
  // contains the necessary data for the summary and detail templates
  if (idle_list.length > 0) {
    // Append the policy metadata to the recommendationDetails which is surfaced in UI and referenced by the Engineering/App Owner personas
    idle_list = _.map(idle_list, function(row) {
      row.recommendationDetails = row.recommendationDetails + "\\n\\n" + idle_analysis_message + lookback_message + disclaimer || '';
      return row;
    });
    idle_list[0]['idle_message'] = idle_message
    idle_list[0]['idle_total_savings'] = idle_total_savings_formatted
    idle_list[0]['policy_name'] = applied_policy_name
  }

  if (underutil_list.length > 0) {
    // Append the policy metadata to the recommendationDetails which is surfaced in UI and referenced by the Engineering/App Owner personas
    underutil_list = _.map(underutil_list, function(row) {
      row.recommendationDetails = row.recommendationDetails + "\\n\\n" + underutil_analysis_message + lookback_message + disclaimer || '';
      return row;
    });
    underutil_list[0]['underutil_message'] = underutil_message
    underutil_list[0]['underutil_total_savings'] = underutil_total_savings_formatted
    underutil_list[0]['policy_name'] = applied_policy_name
  }

  result = {
    "underutil_list": underutil_list,
    "idle_list": idle_list
  }
  EOS
end

datasource "ds_underutilized_instances" do
  run_script $js_underutilized_instances, $ds_idle_and_underutil_instances
end

script "js_underutilized_instances", type: "javascript" do
  parameters "ds_idle_and_underutil_instances"
  result "result"
  code <<-EOS
  result = [{
    "resourceId": "",
    "recommendationType": "",
    "underutil_message": "",
    "idle_message": "",
    "underutil_total_savings": "",
    "idle_total_savings": "",
    "savings": "",
    "savingsCurrency": "",
    "cpuThreshold": "",
    "memoryThreshold": ""
  }]

  if (typeof ds_idle_and_underutil_instances["underutil_list"] != "undefined" && ds_idle_and_underutil_instances["underutil_list"].length > 0) {
    result = ds_idle_and_underutil_instances["underutil_list"].concat(result)
  }
EOS
end

datasource "ds_idle_instances" do
  run_script $js_idle_instances, $ds_idle_and_underutil_instances
end

script "js_idle_instances", type: "javascript" do
  parameters "ds_idle_and_underutil_instances"
  result "result"
  code <<-EOS
  result = [{
    "resourceId": "",
    "recommendationType": "",
    "underutil_message": "",
    "idle_message": "",
    "underutil_total_savings": "",
    "idle_total_savings": "",
    "savings": "",
    "savingsCurrency": "",
    "cpuThreshold": "",
    "memoryThreshold": ""
  }]

  if (typeof ds_idle_and_underutil_instances["idle_list"] != "undefined" && ds_idle_and_underutil_instances["idle_list"].length > 0) {
    result = ds_idle_and_underutil_instances["idle_list"].concat(result)
  }
EOS
end

###############################################################################
# Policy
###############################################################################

policy "pol_databricks_optimizations" do
  validate $ds_verified_tags do
    summary_template "Azure Databricks ClusterId Tag Dimension Not Configured in Flexera."
    detail_template <<-EOS
The **Azure Databricks ClusterId** tag dimension is required to properly allocate costs by mapping each Azure Virtual Machine to its associated Databricks Cluster. This tag dimension is currently missing in your Flexera configuration.

To resolve this, create the required custom tag dimension in Flexera by following the documentation: [**Create a Custom Tag Dimension in Flexera**](https://docs.flexera.com/flexera/EN/Optima/CreatingCustomTags.htm)
EOS
    check eq(val(data, "tag_azure_databricks_clusterid"), 1)
  end
  validate_each $ds_idle_instances do
    summary_template "{{ with index data 0 }}{{ .policy_name }}{{ end }}: {{ len data }} Azure Databricks Idle Virtual Machines Found"
    detail_template <<-'EOS'
    **Potential Monthly Savings:** {{ with index data 0 }}{{ .idle_total_savings }}{{ end }}

    {{ with index data 0 }}{{ .idle_message }}{{ end }}
    EOS
    check logic_or($ds_parent_policy_terminated, eq(val(item, "resourceId"), ""))
    escalate $esc_email
    export do
      resource_level true
      field "savings" do
        label "Estimated Monthly Savings"
      end
      field "savingsCurrency" do
        label "Currency"
      end
      field "recommendationDetails" do
        label "Recommendation"
      end
      field "accountID" do
        label "Subscription ID"
        path "databricks_workspace.subscriptionId"
      end
      field "accountName" do
        label "Subscription Name"
        path "databricks_workspace.subscriptionName"
      end
      field "resourceGroup" do
        label "Azure Resource Group Name"
        path "databricks_workspace.resourceGroup"
      end
      field "service" do
        label "Service Name"
      end
      field "region" do
        label "Azure Region"
        path "databricks_workspace.location"
      end
      field "id" do
        label "Azure Virtual Machine ID"
        path "resourceId"
      end
      field "databricks_workspace_name" do
        label "Databricks Workspace Name"
        path "databricks_workspace.name"
      end
      field "workspaceId" do
        label "Databricks Workspace ID"
        path "databricks_workspace.workspaceId"
      end
      field "databricks_workspace_url" do
        label "Databricks Workspace Url"
        path "databricks_workspace.workspaceUrl"
      end
      field "databricks_cluster_id" do
        label "Databricks Cluster ID"
        path "databricks_cluster_id"
      end
      field "databricks_cluster_name" do
        label "Databricks Cluster Name"
        path "databricks_cluster.cluster_name"
      end
      field "databricks_cluster_source" do
        label "Databricks Cluster Source"
        path "databricks_cluster.cluster_source"
      end
      field "resourceType" do
        label "Instance Size"
        path "resourceType"
      end
      field "newResourceType" do
        label "Recommended Instance Size"
      end
      field "unit_cost" do
        label "Unit Cost during Lookback Period"
      end
      field "total_cost" do
        label "Total Cost during Lookback Period"
        path "total_cost"
      end
      field "cpuMinimum" do
        label "CPU Used % (Minimum)"
        path "metricsForPeriod.cpu_minimum"
      end
      field "cpuAverage" do
        label "CPU Used % (Average)"
        path "metricsForPeriod.cpu_average"
      end
      field "cpuMaximum" do
        label "CPU Used % (Maximum)"
        path "metricsForPeriod.cpu_maximum"
      end
      field "cpuP90" do
        label "CPU Used % (p90)"
        path "metricsForPeriod.cpu_p90"
      end
      field "cpuP95" do
        label "CPU Used % (p95)"
        path "metricsForPeriod.cpu_p95"
      end
      field "cpuP99" do
        label "CPU Used % (p99)"
        path "metricsForPeriod.cpu_p99"
      end
      field "cpuMeasurementCount" do
        label "CPU Measurements Count during Time Period"
        path "metricsForPeriod.cpu_measurement_count"
      end
      field "memMinimum" do
        label "Memory Used % (Minimum)"
        path "metricsForPeriod.memory_minimum_percent"
      end
      field "memAverage" do
        label "Memory Used % (Average)"
        path "metricsForPeriod.memory_average_percent"
      end
      field "memMaximum" do
        label "Memory Used % (Maximum)"
        path "metricsForPeriod.memory_maximum_percent"
      end
      field "memP90" do
        label "Memory Used % (p90)"
        path "metricsForPeriod.memory_p90_percent"
      end
      field "memP95" do
        label "Memory Used % (p95)"
        path "metricsForPeriod.memory_p95_percent"
      end
      field "memP99" do
        label "Memory Used % (p99)"
        path "metricsForPeriod.memory_p99_percent"
      end
      field "memMeasurementCount" do
        label "Memory Measurement Count during Time Period"
        path "metricsForPeriod.memory_measurement_count"
      end
      field "memTotalBytes" do
        label "Memory Total Bytes Available"
        path "metricsForPeriod.memory_total"
      end
      field "thresholdType" do
        label "Threshold Statistic"
      end
      field "thresholdResourceTypes" do
        label "Threshold Resource Type"
      end
      field "cpuThreshold" do
        label "CPU Threshold"
      end
      field "memoryThreshold" do
        label "Memory Threshold"
      end
      field "lookbackPeriod" do
        label "Look Back Period (Days)"
      end
      field "resourceID" do
        label "Resource ID"
        path "resourceId"
      end
      field "chartUrlField" do
        label "Utilization Chart External Link"
        format "link-external"
      end
    end
  end
  validate_each $ds_underutilized_instances do
    summary_template "{{ with index data 0 }}{{ .policy_name }}{{ end }}: {{ len data }} Azure Databricks Underutilized Virtual Machines Found"
    detail_template <<-'EOS'
    **Potential Monthly Savings:** {{ with index data 0 }}{{ .underutil_total_savings }}{{ end }}

    {{ with index data 0 }}{{ .underutil_message }}{{ end }}
    EOS
    check logic_or($ds_parent_policy_terminated, eq(val(item, "resourceId"), ""))
    escalate $esc_email
    export do
      resource_level true
      field "savings" do
        label "Estimated Monthly Savings"
      end
      field "savingsCurrency" do
        label "Currency"
      end
      field "recommendationDetails" do
        label "Recommendation"
      end
      field "accountID" do
        label "Subscription ID"
        path "databricks_workspace.subscriptionId"
      end
      field "accountName" do
        label "Subscription Name"
        path "databricks_workspace.subscriptionName"
      end
      field "resourceGroup" do
        label "Azure Resource Group Name"
        path "databricks_workspace.resourceGroup"
      end
      field "service" do
        label "Service Name"
      end
      field "region" do
        label "Azure Region"
        path "databricks_workspace.location"
      end
      field "id" do
        label "Azure Virtual Machine ID"
        path "resourceId"
      end
      field "databricks_workspace_name" do
        label "Databricks Workspace Name"
        path "databricks_workspace.name"
      end
      field "workspaceId" do
        label "Databricks Workspace ID"
        path "databricks_workspace.workspaceId"
      end
      field "databricks_workspace_url" do
        label "Databricks Workspace Url"
        path "databricks_workspace.workspaceUrl"
      end
      field "databricks_cluster_id" do
        label "Databricks Cluster ID"
        path "databricks_cluster_id"
      end
      field "databricks_cluster_name" do
        label "Databricks Cluster Name"
        path "databricks_cluster.cluster_name"
      end
      field "databricks_cluster_source" do
        label "Databricks Cluster Source"
        path "databricks_cluster.cluster_source"
      end
      field "resourceType" do
        label "Instance Size"
        path "resourceType"
      end
      field "newResourceType" do
        label "Recommended Instance Size"
      end
      field "unit_cost" do
        label "Unit Cost during Lookback Period"
      end
      field "total_cost" do
        label "Total Cost during Lookback Period"
        path "total_cost"
      end
      field "cpuMinimum" do
        label "CPU Used % (Minimum)"
        path "metricsForPeriod.cpu_minimum"
      end
      field "cpuAverage" do
        label "CPU Used % (Average)"
        path "metricsForPeriod.cpu_average"
      end
      field "cpuMaximum" do
        label "CPU Used % (Maximum)"
        path "metricsForPeriod.cpu_maximum"
      end
      field "cpuP90" do
        label "CPU Used % (p90)"
        path "metricsForPeriod.cpu_p90"
      end
      field "cpuP95" do
        label "CPU Used % (p95)"
        path "metricsForPeriod.cpu_p95"
      end
      field "cpuP99" do
        label "CPU Used % (p99)"
        path "metricsForPeriod.cpu_p99"
      end
      field "cpuMeasurementCount" do
        label "CPU Measurements Count during Time Period"
        path "metricsForPeriod.cpu_measurement_count"
      end
      field "memMinimum" do
        label "Memory Used % (Minimum)"
        path "metricsForPeriod.memory_minimum_percent"
      end
      field "memAverage" do
        label "Memory Used % (Average)"
        path "metricsForPeriod.memory_average_percent"
      end
      field "memMaximum" do
        label "Memory Used % (Maximum)"
        path "metricsForPeriod.memory_maximum_percent"
      end
      field "memP90" do
        label "Memory Used % (p90)"
        path "metricsForPeriod.memory_p90_percent"
      end
      field "memP95" do
        label "Memory Used % (p95)"
        path "metricsForPeriod.memory_p95_percent"
      end
      field "memP99" do
        label "Memory Used % (p99)"
        path "metricsForPeriod.memory_p99_percent"
      end
      field "memMeasurementCount" do
        label "Memory Measurement Count during Time Period"
        path "metricsForPeriod.memory_measurement_count"
      end
      field "memTotalBytes" do
        label "Memory Total Bytes Available"
        path "metricsForPeriod.memory_total"
      end
      field "thresholdType" do
        label "Threshold Statistic"
      end
      field "thresholdResourceTypes" do
        label "Threshold Resource Type"
      end
      field "cpuThreshold" do
        label "CPU Threshold"
      end
      field "memoryThreshold" do
        label "Memory Threshold"
      end
      field "lookbackPeriod" do
        label "Look Back Period (Days)"
      end
      field "resourceID" do
        label "Resource ID"
        path "resourceId"
      end
      field "chartUrlField" do
        label "Utilization Chart External Link"
        format "link-external"
      end
    end
  end
end

###############################################################################
# Escalations
###############################################################################

escalation "esc_email" do
  automatic true
  label "Send Email"
  description "Send incident email"
  email $param_email
end

###############################################################################
# Meta Policy [alpha]
# Not intended to be modified or used by policy developers
###############################################################################

# If the meta_parent_policy_id is not set it will evaluate to an empty string and we will look for the policy itself,
# if it is set we will look for the parent policy.
datasource "ds_get_parent_policy" do
  request do
    auth $auth_flexera
    host val($ds_flexera_api_hosts, "flexera")
    path join(["/policy/v1/orgs/", rs_org_id, "/projects/", rs_project_id, "/applied-policies/", switch(ne(meta_parent_policy_id, ""), meta_parent_policy_id, policy_id) ])
	  ignore_status [404]
  end
  result do
    encoding "json"
    field "id", jmes_path(response, "id")
  end
end

# If the policy was applied by a meta_parent_policy we confirm it exists if it doesn't we confirm we are deleting
# This information is used in two places:
# - determining whether or not we make a delete call
# - determining if we should create an incident (we don't want to create an incident on the run where we terminate)
datasource "ds_parent_policy_terminated" do
  run_script $js_parent_policy_terminated, $ds_get_parent_policy, meta_parent_policy_id
end

script "js_parent_policy_terminated", type: "javascript" do
  parameters "ds_get_parent_policy", "meta_parent_policy_id"
  result "result"
  code <<-'EOS'
  result = meta_parent_policy_id != "" && ds_get_parent_policy["id"] == undefined
EOS
end

# Two potentials ways to set this up:
# - this way and make a unneeded 'get' request when not deleting
# - make the delete request an interate and have it iterate over an empty array when not deleting and an array with one item when deleting
datasource "ds_terminate_self" do
  request do
    run_script $js_make_terminate_request, $ds_parent_policy_terminated, $ds_flexera_api_hosts, policy_id, rs_org_id, rs_project_id
  end
end

script "js_make_terminate_request", type: "javascript" do
  parameters "ds_parent_policy_terminated", "ds_flexera_api_hosts", "policy_id", "rs_org_id", "rs_project_id"
  result "request"
  code <<-EOS
  var request = {
    auth: "auth_flexera",
    host: ds_flexera_api_hosts["flexera"],
    path: [ "/policy/v1/orgs/", rs_org_id, "/projects/", rs_project_id, "/applied-policies", policy_id ? "/"+policy_id : "" ].join(''),
    verb: ds_parent_policy_terminated ? "DELETE" : "GET"
  }
EOS
end

# This is just a way to have the check delete request connect to the farthest leaf from policy.
# We want the delete check to the first thing the policy does to avoid the policy erroring before it can decide whether or not it needs to self terminate
# Example a customer deletes a credential and then terminates the parent policy. We still want the children to self terminate
# The only way I could see this not happening is if the user who applied the parent_meta_policy was offboarded or lost policy access, the policies who are impersonating the user
# would not have access to self-terminate
# It may be useful for the backend to enable a mass terminate at some point for all meta_child_policies associated with an id.
datasource "ds_is_deleted" do
  run_script $js_is_deleted, $ds_terminate_self
end

script "js_is_deleted", type: "javascript" do
  parameters "ds_terminate_self"
  result "result"
  code 'result = { path: "/"}'
end
