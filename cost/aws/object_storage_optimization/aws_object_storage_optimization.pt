name "AWS Object Storage Optimization"
rs_pt_ver 20180301
type "policy"
short_description "Check for object store items for last modified date and moves the object to cool or cold archive tiers after user approval. [README](https://github.com/flexera-public/policy_templates/tree/master/cost/aws/object_storage_optimization) and [docs.flexera.com/flexera/EN/Automation](https://docs.flexera.com/flexera/EN/Automation/AutomationGS.htm) to learn more."
long_description ""
category "Cost"
severity "low"
default_frequency "daily"
info(
  version: "3.3",
  provider: "AWS",
  service: "S3",
  policy_set: "Object Store Optimization"
)

###############################################################################
# User inputs
###############################################################################

parameter "param_email" do
  type "list"
  label "Email addresses to notify"
  description "Email addresses of the recipients you wish to notify when new incidents are created"
end

parameter "param_glacier_days" do
  type "string"
  label "Days since last modified to move to Glacier"
  description "Move to glacier after days last modified - leave blank to skip moving"
end

parameter "param_aws_account_number" do
  type "string"
  category "Policy Settings"
  label "Account Number"
  description "Leave blank; this is for automated use with Meta Policies. See README for more details."
  default ""
end

parameter "param_deep_archive_days" do
  type "string"
  label "Days since last modified to move to Deep Archive"
  description "Move to glacier deep archive after days last modified- leave blank to skip moving"
end

parameter "param_exclude_tags" do
  type "list"
  label "Tags to ignore"
  description "List of tags that will exclude s3 objects from being evaluated by this policy. Multiple tags are evaluated as an 'OR' condition. Tag keys or key/value pairs can be listed. Example: 'test,env=dev'"
  default []
end

parameter "param_bucket_list" do
  type "list"
  label "Bucket List"
  description "A list of S3 buckets to assess objects in. Leave blank to assess all buckets."
  default []
end

parameter "param_automatic_action" do
  type "list"
  label "Automatic Actions"
  description "When this value is set, this policy will automatically take the selected action(s)"
  allowed_values ["Update S3 Objects Class"]
  default []
end


###############################################################################
# Authentication
###############################################################################

#authenticate with AWS
credentials "auth_aws" do
  schemes "aws","aws_sts"
  label "AWS"
  description "Select the AWS Credential from the list"
  tags "provider=aws"
  aws_account_number $param_aws_account_number
end

credentials "auth_flexera" do
  schemes "oauth2"
  label "flexera"
  description "Select Flexera One OAuth2 credentials"
  tags "provider=flexera"
end

###############################################################################
# Pagination
###############################################################################

pagination "s3_objects_pagination" do
  get_page_marker do
    body_path "//ListBucketResult/NextContinuationToken"
  end
  set_page_marker do
    query "continuation-token"
  end
end

###############################################################################
# Datasources & Scripts
###############################################################################

#https://docs.aws.amazon.com/AmazonS3/latest/API/RESTServiceGET.html
datasource "ds_aws_buckets" do
  request do
    auth $auth_aws
    host "s3.amazonaws.com"
    path "/"
    # Header X-Meta-Flexera has no affect on datasource query, but is required for Meta Policies
    # Forces `ds_is_deleted` datasource to run first during policy execution
    header "Meta-Flexera", val($ds_is_deleted, "path")
  end
  result do
    encoding "xml"
    collect xpath(response, "//ListAllMyBucketsResult/Buckets/Bucket", "array") do
      field "bucket_name", xpath(col_item,"Name")
      field "bucket_creation_date", xpath(col_item, "CreationDate")
    end
  end
end

datasource "ds_get_caller_identity" do
  request do
    auth $auth_aws
    verb "GET"
    host "sts.amazonaws.com"
    path "/"
    header "User-Agent", "RS Policies"
    query "Action", "GetCallerIdentity"
    query "Version", "2011-06-15"
  end
  result do
    encoding "xml"
    collect xpath(response, "//GetCallerIdentityResponse/GetCallerIdentityResult") do
      field "account",xpath(col_item, "Account")
    end
  end
end

#https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETlocation.html
datasource "ds_aws_buckets_with_region" do
  iterate $ds_aws_buckets
  request do
    auth $auth_aws
    host "s3.amazonaws.com"
    path join(["/", val(iter_item, "bucket_name")])
    query "location", ""
  end
  result do
    encoding "xml"
    field "bucket_name", val(iter_item, "bucket_name")
    field "bucket_creation_date", val(iter_item, "bucket_creation_date")
    field "region", xpath(response, "//LocationConstraint")
  end
end

datasource "ds_aws_sanitized_buckets" do
  run_script $js_parse_buckets, $ds_aws_buckets_with_region, $param_bucket_list
end

script "js_parse_buckets", type: "javascript" do
  parameters "buckets", "param_bucket_list"
  result "results"
  code <<-EOS
    results = []
    bucket_list = []
    if (param_bucket_list.length > 0) {
      bucket_list = _.filter(buckets, function(bucket){ return _.contains(param_bucket_list, bucket.bucket_name)})
    } else {
      bucket_list = buckets
    }

    _.each(bucket_list, function (bucket, i) {
      if ( !bucket["region"] ){
        results.push({
          "bucket_name": bucket["bucket_name"],
          "bucket_creation_date": bucket["bucket_creation_date"],
          "region": "us-east-1",
          "host": "s3.amazonaws.com",
          "tagKeyValue": ""
        })
      }else{
        if(bucket["region"] == "EU" ) { bucket["region"] = "eu-west-1" }
        results.push({
          "bucket_name": bucket["bucket_name"],
          "bucket_creation_date": bucket["bucket_creation_date"],
          "region": bucket["region"],
          "host": "s3-" + bucket["region"] + ".amazonaws.com",
          "tagKeyValue": ""
        })
      }
    });
  EOS
end

#https://docs.aws.amazon.com/AmazonS3/latest/API/v2-RESTBucketGET.html
datasource "ds_aws_list_s3_objects" do
  iterate $ds_aws_sanitized_buckets
  request do
    auth $auth_aws
    pagination $s3_objects_pagination
    verb "GET"
    host val(iter_item, "host")
    path join(["/", val(iter_item, "bucket_name"),"/"])
    query "list-type", "2"
  end
  result do
    encoding "xml"
    collect xpath(response, "//ListBucketResult/Contents", "array") do
      field "bucket_name", val(iter_item, "bucket_name")
      field "region", val(iter_item, "region")
      field "bucket_creation_date", val(iter_item, "bucket_creation_date")
      field "object_key", xpath(col_item,"Key")
      field "storage_class", xpath(col_item,"StorageClass")
      field "last_modified", xpath(col_item,"LastModified")
      field "object_size", xpath(col_item,"Size")
      field "host", val(iter_item, "host")
    end
  end
end

datasource "ds_only_last_modified_objects" do
  run_script $js_only_last_modified_objects, $ds_aws_list_s3_objects, $param_deep_archive_days, $param_glacier_days, $param_exclude_tags
end

script "js_only_last_modified_objects", type: "javascript" do
  parameters "ds_aws_list_s3_objects", "param_deep_archive_days", "param_glacier_days", "param_exclude_tags"
  result "results"
  code <<-EOS
    var results = {
      "require_tags": [],
      "without_tags": []
    }

    var arr = results.without_tags
    if (param_exclude_tags.length > 0){
      arr = results.require_tags
    }

    var glacier_date = new Date();
    var deep_archive_date = new Date();
    var glacier_days_to_move = parseInt(param_glacier_days,10);
    var deep_archive_days_to_move = parseInt(param_deep_archive_days,10);
    glacier_date = glacier_date.setDate(glacier_date.getDate() - glacier_days_to_move);
    deep_archive_date = deep_archive_date.setDate(deep_archive_date.getDate() - deep_archive_days_to_move);
    glacier_date = (new Date(glacier_date)).setHours(23, 59, 59, 999);
    deep_archive_date =(new Date(deep_archive_date)).setHours(23, 59, 59, 999);

    _.each(ds_aws_list_s3_objects, function (s3Object) {
      if (s3Object.storage_class !== "GLACIER" && s3Object.storage_class !== "DEEP_ARCHIVE"){
        var last_modified_date = (new Date(s3Object["last_modified"])).setHours(23, 59, 59, 999);
        var modify_storage_class_to="";
        if(param_glacier_days.length > 0 && param_deep_archive_days.length == 0 &&  last_modified_date <= glacier_date){
          //When param_deep_archive_days is blank.
          modify_storage_class_to = "GLACIER";
        }else if(param_glacier_days.length == 0 && param_deep_archive_days.length > 0 && last_modified_date <= deep_archive_date){
          //When param_glacier_days is blank.
          modify_storage_class_to = "DEEP_ARCHIVE";
        }else if(param_glacier_days.length > 0 && param_deep_archive_days.length > 0 && (last_modified_date <= glacier_date || last_modified_date <= deep_archive_date)){
          if(glacier_days_to_move < deep_archive_days_to_move){
            if(last_modified_date <= glacier_date && last_modified_date >= deep_archive_date){
              modify_storage_class_to = "GLACIER";
            }else{
              modify_storage_class_to = "DEEP_ARCHIVE";
            }
          }else if(glacier_days_to_move > deep_archive_days_to_move){
            if(last_modified_date <= deep_archive_date && last_modified_date >= glacier_date){
              modify_storage_class_to = "DEEP_ARCHIVE";
            }else{
              modify_storage_class_to = "GLACIER";
            }
          }
        }
        if(modify_storage_class_to.length > 0){
          var result = {
              "bucket_name": s3Object["bucket_name"],
              "region":s3Object["region"],
              "bucket_creation_date": s3Object["bucket_creation_date"],
              "host": s3Object["host"],
              "object_key": s3Object["object_key"],
              "storage_class": s3Object["storage_class"],
              "last_modified": s3Object["last_modified"],
              "modify_storage_class_to": modify_storage_class_to,
              "object_size": s3Object["object_size"]
          }
          arr.push(result);
        }
      }
    });
    results.without_tags = _.sortBy(results.without_tags, 'region');
    results.require_tags = _.sortBy(results.require_tags, 'region');
  EOS
end

datasource "ds_require_tags" do
  run_script $js_require_tags, $ds_only_last_modified_objects
end

script "js_require_tags", type: "javascript" do
  parameters "s3objects"
  result "results"
  code <<-EOS
    results = s3objects.require_tags
  EOS
end

datasource "ds_without_tags" do
  run_script $js_without_tags, $ds_only_last_modified_objects
end

script "js_without_tags", type: "javascript" do
  parameters "s3objects"
  result "results"
  code <<-EOS
    results = s3objects.without_tags
  EOS
end

#https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGETtagging.html
datasource "ds_aws_list_s3_objects_with_tags" do
  iterate $ds_require_tags
  request do
    auth $auth_aws
    verb "GET"
    host val(iter_item, "host")
    path join(["/", val(iter_item, "bucket_name"),"/", val(iter_item, "object_key")])
    query "tagging", ""
  end
  result do
    encoding "xml"
    collect xpath(response, "//Tagging/TagSet", "array") do
      field "tags" do
        collect xpath(col_item, "Tag") do
          field "tagKey", xpath(col_item, "Key")
          field "tagValue", xpath(col_item, "Value")
        end
      end
      field "bucket_name", val(iter_item, "bucket_name")
      field "region", val(iter_item, "region")
      field "bucket_creation_date", val(iter_item, "bucket_creation_date")
      field "object_key", val(iter_item, "object_key")
      field "storage_class", val(iter_item, "storage_class")
      field "last_modified", val(iter_item, "last_modified")
      field "object_size", val(iter_item, "object_size")
      field "host", val(iter_item, "host")
      field "modify_storage_class_to", val(iter_item, "modify_storage_class_to")
    end
  end
end

datasource "ds_aws_filtered_s3_objects" do
  run_script $js_aws_filtered_s3_objects, $ds_aws_list_s3_objects_with_tags, $param_exclude_tags, $ds_get_caller_identity, $ds_without_tags
end

#Process the response data, check for the tags and generate a list of s3 objects
script "js_aws_filtered_s3_objects", type: "javascript" do
  parameters "ds_aws_list_s3_objects_with_tags", "param_exclude_tags", "ds_get_caller_identity", "ds_without_tags"
  result "results"
  code <<-EOS
    results = [];
    var param_exclude_tags_lower=[];

    for(var i=0; i < param_exclude_tags.length; i++){
      param_exclude_tags_lower[i]=param_exclude_tags[i].toString().toLowerCase();
    }

    _.each(ds_aws_list_s3_objects_with_tags, function(s3Object){
      var tags = s3Object['tags'];
      var isTagMatched=false
      var tagKeyValue=""
      for(var k=0; k < tags.length; k++){
        tag = tags[k]
        //Check, if the tag present in entered param_exclude_tags, ignore the S3 object if the tag matches/present.
        if((param_exclude_tags_lower.indexOf((tag['tagKey']).toLowerCase()) !== -1) || (param_exclude_tags_lower.indexOf((tag['tagKey']+'='+tag['tagValue']).toLowerCase()) !== -1)){
          isTagMatched = true;
        }
        if((tag['tagValue']).length > 0){
          tagKeyValue = tagKeyValue + ', '+ tag['tagKey']+'='+tag['tagValue']
        }else{
          tagKeyValue = tagKeyValue + ', '+ tag['tagKey']
        }
      }
      if(!isTagMatched){
        results.push({
          "accountId": ds_get_caller_identity[0]['account'],
          "bucket_name": s3Object["bucket_name"],
          "region":s3Object["region"],
          "bucket_creation_date": s3Object["bucket_creation_date"],
          "host": s3Object["host"],
          "object_key": s3Object["object_key"],
          "storage_class": s3Object["storage_class"],
          "last_modified": s3Object["last_modified"],
          "modify_storage_class_to": s3Object["modify_storage_class_to"],
          "object_size": s3Object["object_size"],
          "tagKeyValue":(tagKeyValue.slice(2))
        });
      }
    });

    _.each(ds_without_tags, function(s3Object){
      results.push({
        "accountId": ds_get_caller_identity[0]['account'],
        "bucket_name": s3Object["bucket_name"],
        "region":s3Object["region"],
        "bucket_creation_date": s3Object["bucket_creation_date"],
        "host": s3Object["host"],
        "object_key": s3Object["object_key"],
        "storage_class": s3Object["storage_class"],
        "last_modified": s3Object["last_modified"],
        "modify_storage_class_to": s3Object["modify_storage_class_to"],
        "object_size": s3Object["object_size"],
        "tagKeyValue": ""
      });
    });

    results = _.sortBy(results, 'region');
  EOS
end

###############################################################################
# Policy
###############################################################################

policy "policy_s3_object_list" do
  validate $ds_aws_filtered_s3_objects do
    summary_template "AWS Account ID: {{with index data 0}}{{ .accountId }}{{end}} - {{ len data }} Opportunities for AWS Object Storage Optimization"
    escalate $esc_email
    escalate $esc_modify
    escalate $esc_delete
    check logic_or($ds_parent_policy_terminated, eq(size(data), 0))
    export do
      resource_level true
      field "accountId" do
        label "Account Id"
      end
      field "region" do
        label "Region"
      end
      field "id" do
        label "Bucket Name"
        path "bucket_name"
      end
      field "object_key" do
        label "Object Key"
      end
      field "storage_class" do
        label "Current Storage Class"
      end
      field "last_modified" do
        label "Last Modified Date"
      end
      field "modify_storage_class_to" do
        label "Move Storage Class To"
      end
      field "tagKeyValue" do
        label "Tags"
      end
      field "host" do
        label "Host"
      end
    end
  end
end

###############################################################################
# Escalation
###############################################################################

escalation "esc_email" do
  automatic true
  label "Send Email"
  description "Send incident email"
  email $param_email
end

escalation "esc_modify" do
  automatic contains($param_automatic_action, "Update S3 Objects Class")
  label "Approve S3 Object Storage Class Change"
  description "Approve selected S3 object storage class change"
  run "modify_s3_object_storage_class", data, rs_optima_host
end

escalation "esc_delete" do
  automatic false
  label "Delete S3 Objects"
  description "Delete selected S3 objects"
  run "delete_s3_objects", data, rs_optima_host
end

###############################################################################
# Cloud Workflow
###############################################################################

#https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html
define modify_s3_object_storage_class($data, $$rs_optima_host) return $all_responses do
  $$debug=true
  $all_responses = []
  foreach $item in $data do
    sub on_error: skip do
      call url_encode($item['id']) retrieve $bucket_name
      call url_encode($item['object_key']) retrieve $object_key
      call url_encode($item['id']+"/"+$item['object_key']) retrieve $copy_source
      call url_encode($item['modify_storage_class_to']) retrieve $storage_class
      $response = http_request({
                                verb: 'put',
                                host: $item['host'],
                                auth: $$auth_aws,
                                href: join(['/', $bucket_name, '/', $object_key]),
                                https: true,
                                headers: {'content-type': 'application/x-amz-json-1.1',
                                          'x-amz-copy-source': $copy_source,
                                          'x-amz-storage-class': $storage_class
                                        }
                            })
      $all_responses << $response
      call sys_log('AWS object storage optimization ',to_s($response))
    end
  end
end

#https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectDELETE.html
define delete_s3_objects($data, $$rs_optima_host) return $all_responses do
  $$debug=true
  $all_responses = []
  foreach $item in $data do
    sub on_error: skip do
      call url_encode($item['id']) retrieve $bucket_name
      call url_encode($item['object_key']) retrieve $object_key
    $response = http_delete(url: join(["https://",$item['host'],"/",$bucket_name,"/", $object_key]),
                              auth: $$auth_aws
                            )
      $all_responses << $response
      call sys_log('Delete AWS S3-Objects',to_s($response))
    end
  end
end

define sys_log($subject, $detail) do
  # Create empty errors array if doesn't already exist
  if !$$errors
    $$errors = []
  end
  # Check if debug is enabled
  if $$debug
    # Append to global $$errors
    # This is the suggested way to capture errors
    $$errors << "Unexpected error for " + $subject + "\n  " + to_s($detail)
    # If Flexera NAM Zone, create audit_entries [to be deprecated]
    # This is the legacy method for capturing errors and only supported on Flexera NAM
    if $$rs_optima_host == "api.optima.flexeraeng.com"
      # skip_error_and_append is used to catch error if rs_cm.audit_entries.create fails unexpectedly
      $task_label = "Creating audit entry for " + $subject
      sub task_label: $task, on_error: skip_error_and_append($task) do
        rs_cm.audit_entries.create(
          notify: "None",
          audit_entry: {
            auditee_href: @@account,
            summary: $subject,
            detail: $detail
          }
        )
      end # End sub on_error
    end # End if rs_optima_host
  end # End if debug is enabled
end

define skip_error_and_append($subject) do
  $$errors << "Unexpected error for " + $subject + "\n  " + to_s($_error)
  $_error_behavior = "skip"
end

define url_encode($string) return $encoded_string do
  $encoded_string = $string
  $encoded_string = gsub($encoded_string, " ", "%20")
  $encoded_string = gsub($encoded_string, "!", "%21")
  $encoded_string = gsub($encoded_string, "#", "%23")
  $encoded_string = gsub($encoded_string, "$", "%24")
  $encoded_string = gsub($encoded_string, "&", "%26")
  $encoded_string = gsub($encoded_string, "'", "%27")
  $encoded_string = gsub($encoded_string, "(", "%28")
  $encoded_string = gsub($encoded_string, ")", "%29")
  $encoded_string = gsub($encoded_string, "*", "%2A")
  $encoded_string = gsub($encoded_string, "+", "%2B")
  $encoded_string = gsub($encoded_string, ",", "%2C")
  $encoded_string = gsub($encoded_string, "/", "%2F")
  $encoded_string = gsub($encoded_string, ":", "%3A")
  $encoded_string = gsub($encoded_string, ";", "%3B")
  $encoded_string = gsub($encoded_string, "=", "%3D")
  $encoded_string = gsub($encoded_string, "?", "%3F")
  $encoded_string = gsub($encoded_string, "@", "%40")
  $encoded_string = gsub($encoded_string, "[", "%5B")
  $encoded_string = gsub($encoded_string, "]", "%5D")
end


###############################################################################
# Meta Policy [alpha]
# Not intended to be modified or used by policy developers
###############################################################################

# If the meta_parent_policy_id is not set it will evaluate to an empty string and we will look for the policy itself,
# if it is set we will look for the parent policy.
datasource "ds_get_policy" do
  request do
    auth $auth_flexera
    host rs_governance_host
    ignore_status [404]
    path join(["/api/governance/projects/", rs_project_id, "/applied_policies/", switch(ne(meta_parent_policy_id,""), meta_parent_policy_id, policy_id) ])
    header "Api-Version", "1.0"
  end
  result do
    encoding "json"
    field "id", jmes_path(response, "id")
  end
end


datasource "ds_parent_policy_terminated" do
  run_script $js_decide_if_self_terminate, $ds_get_policy, policy_id, meta_parent_policy_id
end

# If the policy was applied by a meta_parent_policy we confirm it exists if it doesn't we confirm we are deleting
# This information is used in two places:
# - determining whether or not we make a delete call
# - determining if we should create an incident (we don't want to create an incident on the run where we terminate)
script "js_decide_if_self_terminate", type: "javascript" do
  parameters "found", "self_policy_id", "meta_parent_policy_id"
  result "result"
  code <<-EOS
  var result
  if (meta_parent_policy_id != "" && found.id == undefined) {
    result = true
  } else {
    result = false
  }
  EOS
end

# Two potentials ways to set this up:
# - this way and make a unneeded 'get' request when not deleting
# - make the delete request an interate and have it iterate over an empty array when not deleting and an array with one item when deleting
script "js_make_terminate_request", type: "javascript" do
  parameters "should_delete", "policy_id", "rs_project_id", "rs_governance_host"
  result "request"
  code <<-EOS

  var request = {
    auth:  'auth_flexera',
    host: rs_governance_host,
    path: "/api/governance/projects/" + rs_project_id + "/applied_policies/" + policy_id,
    headers: {
      "API-Version": "1.0",
      "Content-Type":"application/json"
    },
  }

  if (should_delete) {
    request.verb = 'DELETE'
  }
  EOS
end

datasource "ds_terminate_self" do
  request do
    run_script $js_make_terminate_request, $ds_parent_policy_terminated, policy_id, rs_project_id, rs_governance_host
  end
end

datasource "ds_is_deleted" do
  run_script $js_check_deleted, $ds_terminate_self
end

# This is just a way to have the check delete request connect to the farthest leaf from policy.
# We want the delete check to the first thing the policy does to avoid the policy erroring before it can decide whether or not it needs to self terminate
# Example a customer deletes a credential and then terminates the parent policy. We still want the children to self terminate
# The only way I could see this not happening is if the user who applied the parent_meta_policy was offboarded or lost policy access, the policies who are impersonating the user
# would not have access to self-terminate
# It may be useful for the backend to enable a mass terminate at some point for all meta_child_policies associated with an id.
script "js_check_deleted", type: "javascript" do
  parameters "response"
  result "result"
  code <<-EOS
  result = {"path":"/"}
  EOS
end
