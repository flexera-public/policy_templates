name "AWS Object Storage Optimization"
rs_pt_ver 20180301
type "policy"
short_description "Check for object store items for last modified date and moves the object to cool or cold archive tiers after user approval. [README](https://github.com/flexera-public/policy_templates/tree/master/cost/aws/object_storage_optimization) and [docs.flexera.com/flexera/EN/Automation](https://docs.flexera.com/flexera/EN/Automation/AutomationGS.htm) to learn more."
long_description ""
category "Cost"
severity "low"
default_frequency "weekly"
info(
  version: "4.0",
  provider: "AWS",
  service: "Storage",
  policy_set: "Object Store Optimization"
)

###############################################################################
# User inputs
###############################################################################

parameter "param_email" do
  type "list"
  category "Policy Settings"
  label "Email Addresses"
  description "A list of email addresses to notify."
  default []
end

parameter "param_aws_account_number" do
  type "string"
  category "Policy Settings"
  label "Account Number"
  description "Leave blank; this is for automated use with Meta Policies. See README for more details."
  default ""
end

parameter "param_bucket_list" do
  type "list"
  category "Filters"
  label "Bucket List"
  description "A list of S3 buckets to assess objects in. Leave blank to assess all buckets."
  default []
end

parameter "param_exclusion_tags" do
  type "list"
  category "Filters"
  label "Exclusion Tags (Key:Value)"
  description "Cloud native tags to ignore resources that you don't want to produce recommendations for. Use Key:Value format for specific tag key/value pairs, and Key:* format to match any resource with a particular key, regardless of value. Examples: env:production, DO_NOT_DELETE:*"
  allowed_pattern /(^$)|[\w]*\:.*/
  default []
end

parameter "param_regions_allow_or_deny" do
  type "string"
  category "Filters"
  label "Allow/Deny Regions"
  description "Allow or Deny entered regions. See the README for more details"
  allowed_values "Allow", "Deny"
  default "Allow"
end

parameter "param_regions_list" do
  type "list"
  category "Filters"
  label "Allow/Deny Regions List"
  allowed_pattern /^([a-zA-Z-_]+-[a-zA-Z0-9-_]+-[0-9-_]+,*|)+$/
  description "A list of allowed or denied regions. See the README for more details"
  default []
end

parameter "param_glacier_days" do
  type "number"
  category "Actions"
  label "Glacier Age Threshold (Days)"
  description "Time in days since object was last modified to change storage class to Glacier. Not applicable if 'Deep Archive' is selected for New Storage Class"
  default 30
  min_value 1
end

parameter "param_deep_archive_days" do
  type "number"
  category "Actions"
  label "Deep Archive Age Threshold (Days)"
  description "Time in days since object was last modified to change storage class to Deep Archive. Not applicable if 'Glacier' is selected for New Storage Class"
  default 90
  min_value 1
end

parameter "param_new_storage_class" do
  type "string"
  category "Actions"
  label "New Storage Class"
  description "Whether to move objects to Glacier or Deep Archive if they meet the specified age thresholds. Select 'Both' to consider moving objects to either one based on the specified age thresholds"
  default "Both"
  allowed_values "Both", "Glacier", "Deep Archive"
end

parameter "param_automatic_action" do
  type "list"
  category "Actions"
  label "Automatic Actions"
  description "When this value is set, this policy will automatically take the selected action."
  allowed_values ["Update S3 Object Storage Class", "Delete S3 Object"]
  default []
end

###############################################################################
# Authentication
###############################################################################

credentials "auth_aws" do
  schemes "aws", "aws_sts"
  label "AWS"
  description "Select the AWS Credential from the list"
  tags "provider=aws"
  aws_account_number $param_aws_account_number
end

credentials "auth_flexera" do
  schemes "oauth2"
  label "Flexera"
  description "Select Flexera One OAuth2 credentials"
  tags "provider=flexera"
end

###############################################################################
# Pagination
###############################################################################

pagination "pagination_aws_s3" do
  get_page_marker do
    body_path "//ListBucketResult/NextContinuationToken"
  end
  set_page_marker do
    query "continuation-token"
  end
end

###############################################################################
# Datasources & Scripts
###############################################################################

## Get applied policy metadata for use later
datasource "ds_applied_policy" do
  request do
    auth $auth_flexera
    host rs_governance_host
    path join(["/api/governance/projects/", rs_project_id, "/applied_policies/", policy_id])
    header "Api-Version", "1.0"
  end
end

# Get region-specific Flexera API endpoints
datasource "ds_flexera_api_hosts" do
  run_script $js_flexera_api_hosts, rs_optima_host
end

script "js_flexera_api_hosts", type: "javascript" do
  parameters "rs_optima_host"
  result "result"
  code <<-EOS
  host_table = {
    "api.optima.flexeraeng.com": {
      flexera: "api.flexera.com",
      fsm: "api.fsm.flexeraeng.com"
    },
    "api.optima-eu.flexeraeng.com": {
      flexera: "api.flexera.eu",
      fsm: "api.fsm-eu.flexeraeng.com"
    },
    "api.optima-apac.flexeraeng.com": {
      flexera: "api.flexera.au",
      fsm: "api.fsm-apac.flexeraeng.com"
    }
  }

  result = host_table[rs_optima_host]
EOS
end

# Get AWS account info
datasource "ds_cloud_vendor_accounts" do
  request do
    auth $auth_flexera
    host val($ds_flexera_api_hosts, 'flexera')
    path join(["/finops-analytics/v1/orgs/", rs_org_id, "/cloud-vendor-accounts"])
    header "Api-Version", "1.0"
  end
  result do
    encoding "json"
    collect jmes_path(response, "values[*]") do
      field "id", jmes_path(col_item, "aws.accountId")
      field "name", jmes_path(col_item, "name")
      field "tags", jmes_path(col_item, "tags")
    end
  end
end

datasource "ds_get_caller_identity" do
  request do
    auth $auth_aws
    verb "GET"
    host "sts.amazonaws.com"
    path "/"
    header "User-Agent", "RS Policies"
    query "Action", "GetCallerIdentity"
    query "Version", "2011-06-15"
  end
  result do
    encoding "xml"
    collect xpath(response, "//GetCallerIdentityResponse/GetCallerIdentityResult") do
      field "account", xpath(col_item, "Account")
    end
  end
end

datasource "ds_aws_account" do
  run_script $js_aws_account, $ds_cloud_vendor_accounts, $ds_get_caller_identity
end

script "js_aws_account", type:"javascript" do
  parameters "ds_cloud_vendor_accounts", "ds_get_caller_identity"
  result "result"
  code <<-EOS
  result = _.find(ds_cloud_vendor_accounts, function(account) {
    return account['id'] == ds_get_caller_identity[0]['account']
  })

  // This is in case the API does not return the relevant account info
  if (result == undefined) {
    result = {
      id: ds_get_caller_identity[0]['account'],
      name: "",
      tags: {}
    }
  }
EOS
end

datasource "ds_get_aws_buckets" do
  request do
    auth $auth_aws
    host "s3.amazonaws.com"
    path "/"
    # Header X-Meta-Flexera has no affect on datasource query, but is required for Meta Policies
    # Forces `ds_is_deleted` datasource to run first during policy execution
    header "Meta-Flexera", val($ds_is_deleted, "path")
  end
  result do
    encoding "xml"
    collect xpath(response, "//ListAllMyBucketsResult/Buckets/Bucket", "array") do
      field "name", xpath(col_item,"Name")
      field "creation_date", xpath(col_item, "CreationDate")
    end
  end
end

datasource "ds_aws_buckets_name_filtered" do
  run_script $js_aws_buckets_name_filtered, $ds_get_aws_buckets, $param_bucket_list
end

script "js_aws_buckets_name_filtered", type: "javascript" do
  parameters "ds_get_aws_buckets", "param_bucket_list"
  result "result"
  code <<-EOS
  if (param_bucket_list.length > 0) {
    result = _.filter(ds_get_aws_buckets, function(bucket) {
      return _.contains(param_bucket_list, bucket['name'])
    })
  } else {
    result = ds_get_aws_buckets
  }
EOS
end

datasource "ds_aws_buckets_with_region" do
  iterate $ds_aws_buckets_name_filtered
  request do
    auth $auth_aws
    host "s3.amazonaws.com"
    path join(["/", val(iter_item, "name")])
    query "location", ""
  end
  result do
    encoding "xml"
    field "region", xpath(response, "//LocationConstraint")
    field "name", val(iter_item, "name")
    field "creation_date", val(iter_item, "creation_date")
  end
end

datasource "ds_aws_buckets_region_filtered" do
  run_script $js_aws_buckets_region_filtered, $ds_aws_buckets_with_region, $param_regions_allow_or_deny, $param_regions_list
end

script "js_aws_buckets_region_filtered", type: "javascript" do
  parameters "ds_aws_buckets_with_region", "param_regions_allow_or_deny", "param_regions_list"
  result "result"
  code <<-EOS
  if (param_regions_list.length > 0) {
    result = _.filter(ds_aws_buckets_with_region, function(bucket) {
      include_bucket = _.contains(param_regions_list, bucket['region'])

      if (param_regions_allow_or_deny == "Deny") {
        include_bucket = !include_bucket
      }

      return include_bucket
    })
  } else {
    result = ds_azure_instances_tag_filtered
  }
EOS
end

datasource "ds_aws_buckets" do
  run_script $js_aws_buckets, $ds_aws_buckets_region_filtered
end

script "js_aws_buckets", type: "javascript" do
  parameters "ds_aws_buckets_region_filtered"
  result "result"
  code <<-'EOS'
  result = _.map(ds_aws_buckets_region_filtered, function(bucket) {
    if (typeof(bucket['region']) != 'string' || bucket['region'] == '' || bucket['region'] == 'us-east-1') {
      region = 'us-east-1'
      host = 's3.amazonaws.com'
    } else if (bucket['region'] == 'EU') {
      region = 'eu-west-1'
      host = 's3-eu-west-1.amazonaws.com'
    } else {
      region = bucket['region'].toLowerCase().trim()
      host = 's3-' + region + '.amazonaws.com'
    }

    return {
      name: bucket['name'],
      creation_date: bucket['creation_date'],
      region: region,
      host: host
    }
  })
EOS
end

datasource "ds_aws_list_s3_objects" do
  iterate $ds_aws_buckets
  request do
    auth $auth_aws
    pagination $pagination_aws_s3
    verb "GET"
    host val(iter_item, "host")
    path join(["/", val(iter_item, "name"), "/"])
    query "list-type", "2"
  end
  result do
    encoding "xml"
    collect xpath(response, "//ListBucketResult/Contents", "array") do
      field "object_key", xpath(col_item, "Key")
      field "storage_class", xpath(col_item, "StorageClass")
      field "last_modified", xpath(col_item, "LastModified")
      field "object_size", xpath(col_item, "Size")
      field "bucket_name", val(iter_item, "name")
      field "bucket_creation_date", val(iter_item, "creation_date")
      field "region", val(iter_item, "region")
      field "host", val(iter_item, "host")
    end
  end
end

datasource "ds_aws_s3_objects_tag_queries" do
  run_script $js_aws_s3_objects_tag_queries, $ds_aws_list_s3_objects, $param_exclusion_tags
end

script "js_aws_s3_objects_tag_queries", type: "javascript" do
  parameters "ds_aws_list_s3_objects", "param_exclusion_tags"
  result "result"
  code <<-'EOS'
  if (param_exclusion_tags.length > 0) {
    result = ds_aws_list_s3_objects
  } else {
    result = []
  }
EOS
end

datasource "ds_aws_s3_objects_with_tags" do
  iterate $ds_aws_s3_objects_tag_queries
  request do
    auth $auth_aws
    verb "GET"
    host val(iter_item, "host")
    path join(["/", val(iter_item, "bucket_name"), "/", val(iter_item, "object_key")])
    query "tagging", ""
  end
  result do
    encoding "xml"
    collect xpath(response, "//Tagging/TagSet", "array") do
      field "tags" do
        collect xpath(col_item, "Tag") do
          field "key", xpath(col_item, "Key")
          field "value", xpath(col_item, "Value")
        end
      end
      field "object_key", val(iter_item, "object_key")
      field "storage_class", val(iter_item, "storage_class")
      field "last_modified", val(iter_item, "last_modified")
      field "object_size", val(iter_item, "object_size")
      field "bucket_name", val(iter_item, "bucket_name")
      field "bucket_creation_date", val(iter_item, "bucket_creation_date")
      field "region", val(iter_item, "region")
      field "host", val(iter_item, "host")
    end
  end
end

datasource "ds_aws_s3_objects" do
  run_script $js_aws_s3_objects, $ds_aws_list_s3_objects, $ds_aws_s3_objects_with_tags, $param_exclusion_tags
end

script "js_aws_s3_objects", type: "javascript" do
  parameters "ds_aws_list_s3_objects", "ds_aws_s3_objects_with_tags", "param_exclusion_tags"
  result "result"
  code <<-'EOS'
  if (param_exclusion_tags.length > 0) {
    result = _.reject(ds_aws_s3_objects_with_tags, function(object) {
      object_tags = []

      if (object['tags'] != null && object['tags'] != undefined) {
        _.each(object['tags'], function(tag) {
          object_tags.push([tag['key'], tag['value']].join(':'))
          object_tags.push([tag['key'], '*'].join(':'))
        })
      }

      exclude_object = false

      _.each(param_exclusion_tags, function(exclusion_tag) {
        if (_.contains(object_tags, exclusion_tag)) {
          exclude_object = true
        }
      })

      return exclude_object
    })
  } else {
    result = _.map(ds_aws_list_s3_objects, function(object) {
      return {
        object_key: object['object_key'],
        storage_class: object['storage_class'],
        last_modified: object['last_modified'],
        object_size: object['object_size'],
        bucket_name: object['bucket_name'],
        bucket_creation_date: object['bucket_creation_date'],
        region: object['region'],
        host: object['host'],
        tags: []
      }
    })
  }
EOS
end

datasource "ds_aws_s3_objects_with_storage_class" do
  run_script $js_aws_s3_objects_with_storage_class, $ds_aws_list_s3_objects, $param_deep_archive_days, $param_glacier_days, $param_new_storage_class
end

script "js_aws_s3_objects_with_storage_class", type: "javascript" do
  parameters "ds_aws_list_s3_objects", "param_deep_archive_days", "param_glacier_days", "param_new_storage_class"
  result "result"
  code <<-'EOS'
  result = _.map(ds_aws_list_s3_objects, function(object) {
    object_tags = []

    if (object['tags'] != null && object['tags'] != undefined) {
      _.each(object['tags'], function(tag) {
        object_tags.push([tag['key'], tag['value']].join('='))
      })
    }

    last_modified_date = new Date(object['last_modified'])
    glacier_date = new Date(new Date() - (1000 * 60 * 60 * 24 * param_glacier_days))
    deep_archive_date = new Date(new Date() - (1000 * 60 * 60 * 24 * param_deep_archive_days))
    new_storage_class = null

    if (object['storage_class'] != "GLACIER" && object['storage_class'] != "DEEP_ARCHIVE") {
      if (last_modified_date <= glacier_date && param_new_storage_class != 'Deep Archive') {
        new_storage_class = "GLACIER"
      }

      if (last_modified_date <= deep_archive_date && param_new_storage_class != 'Glacier') {
        new_storage_class = "DEEP_ARCHIVE"
      }
    }

    return {
      object_key: object['object_key'],
      storage_class: object['storage_class'],
      object_size: object['object_size'],
      bucket_name: object['bucket_name'],
      bucket_creation_date: object['bucket_creation_date'],
      region: object['region'],
      host: object['host'],
      tags: object_tags.join(', '),
      last_modified: last_modified_date.toISOString(),
      new_storage_class: new_storage_class
    }
  })
EOS
end


datasource "ds_aws_s3_objects_incident" do
  run_script $js_aws_s3_objects_incident, $ds_aws_s3_objects_with_storage_class, $ds_aws_account, $ds_applied_policy, $param_deep_archive_days, $param_glacier_days, $param_new_storage_class
end

script "js_aws_s3_objects_incident", type: "javascript" do
  parameters "ds_aws_s3_objects_with_storage_class", "ds_aws_account", "ds_applied_policy", "param_deep_archive_days", "param_glacier_days", "param_new_storage_class"
  result "result"
  code <<-'EOS'
  objects_to_change = _.reject(ds_aws_s3_objects_with_storage_class, function(object) {
    return object['new_storage_class'] == null
  })

  result = _.map(objects_to_change, function(object) {
    recommendationDetails = [
      "Change storage class of S3 Object ", object['object_key'],
      " in S3 Bucket ", object['bucket_name'],
      " in AWS Account ", ds_aws_account['name'], " (", ds_aws_account['id'], ")",
      " from ", object['storage_class'], " to ", object['new_storage_class']
    ].join('')

    return {
      resourceID: object['object_key'],
      resourceType: object['storage_class'],
      newResourceType: object['new_storage_class'],
      object_size: object['object_size'],
      bucket_name: object['bucket_name'],
      bucket_creation_date: object['bucket_creation_date'],
      region: object['region'],
      host: object['host'],
      tags: object['tags'],
      last_modified: object['last_modified'],
      accountID: ds_aws_account['id'],
      accountName: ds_aws_account['name'],
      policy_name: ds_applied_policy['name'],
      recommendationDetails: recommendationDetails,
      message: ''
    }
  })

  objects_total = ds_aws_s3_objects_with_storage_class.length.toString()
  objects_to_change_total = result.length.toString()
  objects_to_change_percentage = (objects_to_change_total / objects_total * 100).toFixed(2).toString() + '%'

  object_noun = "objects"
  if (objects_total == 1) { object_noun = "object" }

  object_verb = "are"
  if (objects_to_change_total == 1) { object_verb = "is" }

  findings = [
    "Out of ", objects_total, " AWS S3 ", object_noun, " analyzed, ",
    objects_to_change_total, " (", objects_to_change_percentage,
    ") ", object_verb, " recommended for a change in storage class. "
  ].join('')

  analysis = ''

  if (param_new_storage_class != 'Deep Archive') {
    glacier_day_noun = "days ago"
    if (param_glacier_days == 1) { glacier_day_noun = "day ago" }

    analysis += [
      "An S3 Object is recommended for a change to the Glacier storage class ",
      "if it was last modified at least ", param_glacier_days, " ", glacier_day_noun, ". "
    ].join('')
  }

  if (param_new_storage_class != 'Glacier') {
    deep_day_noun = "days ago"
    if (param_deep_archive_days == 1) { deep_day_noun = "day ago" }

    analysis += [
      "An S3 Object is recommended for a change to the Deep Archive storage class ",
      "if it was last modified at least ", param_deep_archive_days, " ", deep_day_noun, "."
    ].join('')
  }

  analysis += "\n\n"

  disclaimer = "The above settings can be modified by editing the applied policy and changing the appropriate parameters."

  // Dummy entry to ensure validation runs at least once
  result.push({ resourceID: "", policy_name: "", message: "", tags: "" })

  result[0]['message'] = findings + analysis + disclaimer
EOS
end


###############################################################################
# Policy
###############################################################################

policy "policy_s3_object_list" do
  validate_each $ds_aws_s3_objects_incident do
    summary_template "{{ with index data 0 }}{{ .policy_name }}{{ end }}: {{ len data }} AWS S3 Objects Recommended For Storage Class Change"
    detail_template "{{ with index data 0 }}{{ .message }}{{ end }}"
    # Policy check fails and incident is created only if data is not empty and the Parent Policy has not been terminated
    check logic_or($ds_parent_policy_terminated, eq(val(item, "resourceID"), ""))
    escalate $esc_email
    escalate $esc_update
    escalate $esc_delete
    hash_exclude "message", "tags"
    export do
      resource_level true
      field "accountID" do
        label "Account ID"
      end
      field "accountName" do
        label "Account Name"
      end
      field "bucket_name" do
        label "Bucket Name"
      end
      field "resourceID" do
        label "Object Key"
      end
      field "region" do
        label "Region"
      end
      field "last_modified" do
        label "Last Modified Date"
      end
      field "recommendationDetails" do
        label "Recommendation"
      end
      field "resourceType" do
        label "Current Storage Class"
      end
      field "newResourceType" do
        label "Recommended Storage Class"
      end
      field "tags" do
        label "Tags"
      end
      field "host" do
        label "Host"
      end
      field "id" do
        label "ID"
        path "resourceID"
      end
    end
  end
end

###############################################################################
# Escalation
###############################################################################

escalation "esc_email" do
  automatic true
  label "Send Email"
  description "Send incident email"
  email $param_email
end

escalation "esc_update" do
  automatic contains($param_automatic_action, "Update S3 Object Storage Class")
  label "Update S3 Objects Storage Class"
  description "Approval to update the storage class of all selected S3 Objects"
  run "update_objects", data
end

escalation "esc_delete" do
  automatic contains($param_automatic_action, "Delete S3 Object")
  label "Delete S3 Objects"
  description "Approval to delete all selected S3 Objects"
  run "delete_objects", data
end

###############################################################################
# Cloud Workflow
###############################################################################

define update_objects($data) return $all_responses do
  $$all_responses = []

  foreach $object in $data do
    sub on_error: handle_error() do
      call update_object($object) retrieve $update_response
      $$all_responses << $update_response
    end
  end

  if inspect($$errors) != "null"
    raise join($$errors,"\n")
  end
end

define update_object($object) return $response do
  call url_encode($object['bucket_name']) retrieve $bucket_name
  call url_encode($object['resourceID']) retrieve $object_key
  call url_encode($object['bucket_name'] + '/' + $object['resourceID']) retrieve $copy_source
  call url_encode($object['newResourceType']) retrieve $storage_class

  $host = $object['host']
  $href = '/' + $bucket_name + '/' + $object_key
  $url = $host + $href
  task_label("PUT " + $url)

  $response = http_request(
    auth: $$auth_aws,
    https: true,
    verb: "put",
    href: $href,
    host: $host,
    headers: {
      'content-type': 'application/x-amz-json-1.1',
      'x-amz-copy-source': $copy_source,
      'x-amz-storage-class': $storage_class
    }
  )

  task_label("Put AWS S3 Object response: " + $object['resourceID'] + " " + to_json($response))
  $$all_responses << to_json({"req": "PUT " + $url, "resp": $response})

  if $response["code"] != 200 && $response["code"] != 202 && $response["code"] != 204
    raise "Unexpected response putting AWS S3 Object: "+ $object['resourceID'] + " " + to_json($response)
  else
    task_label("Put AWS S3 Object successful: " + $object['resourceID'])
  end
end

define delete_objects($data) return $all_responses do
  $$all_responses = []

  foreach $object in $data do
    sub on_error: handle_error() do
      call delete_object($object) retrieve $delete_response
      $$all_responses << $delete_response
    end
  end

  if inspect($$errors) != "null"
    raise join($$errors,"\n")
  end
end

define delete_object($object) return $response do
  call url_encode($object['bucket_name']) retrieve $bucket_name
  call url_encode($object['resourceID']) retrieve $object_key

  $host = $object['host']
  $href = '/' + $bucket_name + '/' + $object_key
  $url = $host + $href
  task_label("DELETE " + $url)

  $response = http_request(
    auth: $$auth_aws,
    https: true,
    verb: "delete",
    href: $href,
    host: $host
  )

  task_label("Delete AWS S3 Object response: " + $object['resourceID'] + " " + to_json($response))
  $$all_responses << to_json({"req": "DELETE " + $url, "resp": $response})

  if $response["code"] != 200 && $response["code"] != 202 && $response["code"] != 204
    raise "Unexpected response deleting AWS S3 Object: "+ $object['resourceID'] + " " + to_json($response)
  else
    task_label("Delete AWS S3 Object successful: " + $object['resourceID'])
  end
end

define url_encode($string) return $encoded_string do
  $encoded_string = $string
  $encoded_string = gsub($encoded_string, " ", "%20")
  $encoded_string = gsub($encoded_string, "!", "%21")
  $encoded_string = gsub($encoded_string, "#", "%23")
  $encoded_string = gsub($encoded_string, "$", "%24")
  $encoded_string = gsub($encoded_string, "&", "%26")
  $encoded_string = gsub($encoded_string, "'", "%27")
  $encoded_string = gsub($encoded_string, "(", "%28")
  $encoded_string = gsub($encoded_string, ")", "%29")
  $encoded_string = gsub($encoded_string, "*", "%2A")
  $encoded_string = gsub($encoded_string, "+", "%2B")
  $encoded_string = gsub($encoded_string, ",", "%2C")
  $encoded_string = gsub($encoded_string, "/", "%2F")
  $encoded_string = gsub($encoded_string, ":", "%3A")
  $encoded_string = gsub($encoded_string, ";", "%3B")
  $encoded_string = gsub($encoded_string, "=", "%3D")
  $encoded_string = gsub($encoded_string, "?", "%3F")
  $encoded_string = gsub($encoded_string, "@", "%40")
  $encoded_string = gsub($encoded_string, "[", "%5B")
  $encoded_string = gsub($encoded_string, "]", "%5D")
end

define handle_error() do
  if !$$errors
    $$errors = []
  end
  $$errors << $_error["type"] + ": " + $_error["message"]
  # We check for errors at the end, and raise them all together
  # Skip errors handled by this definition
  $_error_behavior = "skip"
end

###############################################################################
# Meta Policy [alpha]
# Not intended to be modified or used by policy developers
###############################################################################

# If the meta_parent_policy_id is not set it will evaluate to an empty string and we will look for the policy itself,
# if it is set we will look for the parent policy.
datasource "ds_get_policy" do
  request do
    auth $auth_flexera
    host rs_governance_host
    ignore_status [404]
    path join(["/api/governance/projects/", rs_project_id, "/applied_policies/", switch(ne(meta_parent_policy_id,""), meta_parent_policy_id, policy_id) ])
    header "Api-Version", "1.0"
  end
  result do
    encoding "json"
    field "id", jmes_path(response, "id")
  end
end

datasource "ds_parent_policy_terminated" do
  run_script $js_decide_if_self_terminate, $ds_get_policy, policy_id, meta_parent_policy_id
end

# If the policy was applied by a meta_parent_policy we confirm it exists if it doesn't we confirm we are deleting
# This information is used in two places:
# - determining whether or not we make a delete call
# - determining if we should create an incident (we don't want to create an incident on the run where we terminate)
script "js_decide_if_self_terminate", type: "javascript" do
  parameters "found", "self_policy_id", "meta_parent_policy_id"
  result "result"
  code <<-EOS
  var result
  if (meta_parent_policy_id != "" && found.id == undefined) {
    result = true
  } else {
    result = false
  }
  EOS
end

# Two potentials ways to set this up:
# - this way and make a unneeded 'get' request when not deleting
# - make the delete request an interate and have it iterate over an empty array when not deleting and an array with one item when deleting
script "js_make_terminate_request", type: "javascript" do
  parameters "should_delete", "policy_id", "rs_project_id", "rs_governance_host"
  result "request"
  code <<-EOS

  var request = {
    auth:  'auth_flexera',
    host: rs_governance_host,
    path: "/api/governance/projects/" + rs_project_id + "/applied_policies/" + policy_id,
    headers: {
      "API-Version": "1.0",
      "Content-Type":"application/json"
    },
  }

  if (should_delete) {
    request.verb = 'DELETE'
  }
  EOS
end

datasource "ds_terminate_self" do
  request do
    run_script $js_make_terminate_request, $ds_parent_policy_terminated, policy_id, rs_project_id, rs_governance_host
  end
end

datasource "ds_is_deleted" do
  run_script $js_check_deleted, $ds_terminate_self
end

# This is just a way to have the check delete request connect to the farthest leaf from policy.
# We want the delete check to the first thing the policy does to avoid the policy erroring before it can decide whether or not it needs to self terminate
# Example a customer deletes a credential and then terminates the parent policy. We still want the children to self terminate
# The only way I could see this not happening is if the user who applied the parent_meta_policy was offboarded or lost policy access, the policies who are impersonating the user
# would not have access to self-terminate
# It may be useful for the backend to enable a mass terminate at some point for all meta_child_policies associated with an id.
script "js_check_deleted", type: "javascript" do
  parameters "response"
  result "result"
  code <<-EOS
  result = {"path":"/"}
  EOS
end
