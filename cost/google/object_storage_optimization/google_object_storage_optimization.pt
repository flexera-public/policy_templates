name "Google Object Storage Optimization"
rs_pt_ver 20180301
type "policy"
short_description "Checks Google Object Storage for old objects and moves said objects to the 'nearline' or 'coldline' storage classes after user approval. See the [README](https://github.com/flexera-public/policy_templates/tree/master/cost/google/object_storage_optimization) and [docs.flexera.com/flexera/EN/Automation](https://docs.flexera.com/flexera/EN/Automation/AutomationGS.htm) to learn more."
long_description ""
severity "low"
category "Cost"
default_frequency "weekly"
info(
  version: "3.0.0",
  provider: "Google",
  service: "Storage",
  policy_set: "Object Store Optimization"
)

###############################################################################
# Parameters
###############################################################################

parameter "param_email" do
  type "list"
  category "Policy Settings"
  label "Email Addresses"
  description "A list of email addresses to notify."
  default []
end

parameter "param_storage_bucket_list" do
  type "list"
  category "Filters"
  label "Storage Bucket List"
  description "A list of Google Object Storage Buckets to assess objects in. Leave blank to assess objects in all buckets."
  default []
end

parameter "param_projects_allow_or_deny" do
  type "string"
  category "Filters"
  label "Allow/Deny Projects"
  description "Allow or Deny entered Projects. See the README for more details."
  allowed_values "Allow", "Deny"
  default "Allow"
end

parameter "param_projects_list" do
  type "list"
  category "Filters"
  label "Allow/Deny Projects List"
  description "A list of allowed or denied Project IDs/names. See the README for more details."
  default []
end

parameter "param_exclusion_labels" do
  type "list"
  category "Filters"
  label "Exclusion Labels"
  description "Cloud native labels to ignore storage buckets that you don't want to produce recommendations for. Enter the Key name to filter resources with a specific Key, regardless of Value, and enter Key==Value to filter resources with a specific Key:Value pair. Other operators and regex are supported; please see the README for more details."
  default []
end

parameter "param_exclusion_labels_boolean" do
  type "string"
  category "Filters"
  label "Exclusion Labels: Any / All"
  description "Whether to filter storage buckets containing any of the specified labels or only those that contain all of them. Only applicable if more than one value is entered in the 'Exclusion Labels' field."
  allowed_values "Any", "All"
  default "Any"
end

parameter "param_new_storage_class" do
  type "string"
  category "Actions"
  label "New Storage Class"
  description "Whether to move objects to 'nearline' or 'coldline' if they meet the specified age thresholds. Select 'Both' to consider moving objects to either one based on the specified age thresholds"
  allowed_values "Both", "Nearline", "Coldline"
  default "Both"
end

parameter "param_nearline_days" do
  type "number"
  category "Actions"
  label "Nearline Class Age Threshold (Days)"
  description "Time in days since object was last modified to change storage tier to 'nearline'. Not applicable if 'Coldline' is selected for New Storage Class"
  min_value 1
  default 30
end

parameter "param_coldline_days" do
  type "number"
  category "Actions"
  label "Coldline Class Age Threshold (Days)"
  description "Time in days since object was last modified to change storage tier to 'coldline'. Not applicable if 'Nearline' is selected for New Storage Class"
  min_value 1
  default 90
end

parameter "param_automatic_action" do
  type "list"
  category "Actions"
  label "Automatic Actions"
  description "When this value is set, this policy will automatically take the selected action."
  allowed_values ["Update Objects Storage Class", "Delete Objects"]
  default []
end

###############################################################################
# Authentication
###############################################################################

credentials "auth_google" do
  schemes "oauth2"
  label "Google"
  description "Select the Google Cloud Credential from the list."
  tags "provider=gce"
end

credentials "auth_flexera" do
  schemes "oauth2"
  label "Flexera"
  description "Select Flexera One OAuth2 credentials"
  tags "provider=flexera"
end

###############################################################################
# Pagination
###############################################################################

pagination "pagination_google" do
  get_page_marker do
    body_path "nextPageToken"
  end
  set_page_marker do
    query "pageToken"
  end
end

###############################################################################
# Datasources & Scripts
###############################################################################

# Get applied policy metadata for use later
datasource "ds_applied_policy" do
  request do
    auth $auth_flexera
    host rs_governance_host
    path join(["/api/governance/projects/", rs_project_id, "/applied_policies/", policy_id])
    header "Api-Version", "1.0"
  end
end

datasource "ds_google_projects" do
  request do
    auth $auth_google
    pagination $pagination_google
    host "cloudresourcemanager.googleapis.com"
    path "/v1/projects/"
    query "filter", "(lifecycleState:ACTIVE)"
    # Header X-Meta-Flexera has no affect on datasource query, but is required for Meta Policies
    # Forces `ds_is_deleted` datasource to run first during policy execution
    header "Meta-Flexera", val($ds_is_deleted, "path")
  end
  result do
    encoding "json"
    collect jmes_path(response, "projects[*]") do
      field "number", jmes_path(col_item, "projectNumber")
      field "id", jmes_path(col_item, "projectId")
      field "name", jmes_path(col_item, "name")
    end
  end
end

datasource "ds_google_projects_filtered" do
  run_script $js_google_projects_filtered, $ds_google_projects, $param_projects_allow_or_deny, $param_projects_list
end

script "js_google_projects_filtered", type: "javascript" do
  parameters "ds_google_projects", "param_projects_allow_or_deny", "param_projects_list"
  result "result"
  code <<-EOS
  if (param_projects_list.length > 0) {
    result = _.filter(ds_google_projects, function(project) {
      include_project = _.contains(param_projects_list, project['id']) || _.contains(param_projects_list, project['name']) || _.contains(param_projects_list, project['number'])

      if (param_projects_allow_or_deny == "Deny") {
        include_project = !include_project
      }

      return include_project
    })
  } else {
    result = ds_google_projects
  }
EOS
end

datasource "ds_google_buckets" do
  iterate $ds_google_projects_filtered
  request do
    auth $auth_google
    pagination $pagination_google
    verb "GET"
    host "storage.googleapis.com"
    path "/storage/v1/b"
    query "project", val(iter_item, "id")
    query "projection", "noAcl"
    ignore_status [403, 404]
  end
  result do
    encoding "json"
    collect jmes_path(response, "items[*]") do
      field "id", jmes_path(col_item, "id")
      field "name", jmes_path(col_item, "name")
      field "labels", jmes_path(col_item, "labels")
      field "location", jmes_path(col_item, "location")
      field "locationType", jmes_path(col_item, "locationType")
      field "storageClass", jmes_path(col_item, "storageClass")
      field "timeCreated", jmes_path(col_item, "timeCreated")
      field "projectId", val(iter_item, "id")
      field "projectName", val(iter_item, "name")
      field "projectNumber", val(iter_item, "projectNumber")
    end
  end
end

datasource "ds_google_buckets_label_filtered" do
  run_script $js_google_buckets_label_filtered, $ds_google_buckets, $param_exclusion_labels, $param_exclusion_labels_boolean
end

script "js_google_buckets_label_filtered", type: "javascript" do
  parameters "ds_google_buckets", "param_exclusion_labels", "param_exclusion_labels_boolean"
  result "result"
  code <<-EOS
  comparators = _.map(param_exclusion_labels, function(item) {
    if (item.indexOf('==') != -1) {
      return { comparison: '==', key: item.split('==')[0], value: item.split('==')[1], string: item }
    }

    if (item.indexOf('!=') != -1) {
      return { comparison: '!=', key: item.split('!=')[0], value: item.split('!=')[1], string: item }
    }

    if (item.indexOf('=~') != -1) {
      value = item.split('=~')[1]
      regex = new RegExp(value.slice(1, value.length - 1))
      return { comparison: '=~', key: item.split('=~')[0], value: regex, string: item }
    }

    if (item.indexOf('!~') != -1) {
      value = item.split('!~')[1]
      regex = new RegExp(value.slice(1, value.length - 1))
      return { comparison: '!~', key: item.split('!~')[0], value: regex, string: item }
    }

    // If = is present but none of the above are, assume user error and that the user intended ==
    if (item.indexOf('=') != -1) {
      return { comparison: '==', key: item.split('=')[0], value: item.split('=')[1], string: item }
    }

    // Assume we're just testing for a key if none of the comparators are found
    return { comparison: 'key', key: item, value: null, string: item }
  })

  if (param_exclusion_labels.length > 0) {
    result = _.reject(ds_google_buckets, function(resource) {
      resource_labels = {}
      if (typeof(resource['labels']) == 'object') { resource_labels = resource['labels'] }

      // Store a list of found labels
      found_labels = []

      _.each(comparators, function(comparator) {
        comparison = comparator['comparison']
        value = comparator['value']
        string = comparator['string']
        resource_label = resource_labels[comparator['key']]

        if (comparison == 'key' && resource_label != undefined) { found_labels.push(string) }
        if (comparison == '==' && resource_label == value) { found_labels.push(string) }
        if (comparison == '!=' && resource_label != value) { found_labels.push(string) }

        if (comparison == '=~') {
          if (resource_label != undefined && value.test(resource_label)) { found_labels.push(string) }
        }

        if (comparison == '!~') {
          if (resource_label == undefined) { found_labels.push(string) }
          if (resource_label != undefined && value.test(resource_label)) { found_labels.push(string) }
        }
      })

      all_labels_found = found_labels.length == comparators.length
      any_labels_found = found_labels.length > 0 && param_exclusion_labels_boolean == 'Any'

      return all_labels_found || any_labels_found
    })
  } else {
    result = ds_google_buckets
  }
EOS
end

datasource "ds_google_buckets_name_filtered" do
  run_script $js_google_buckets_name_filtered, $ds_google_buckets_label_filtered, $param_storage_bucket_list
end

script "js_google_buckets_name_filtered", type: "javascript" do
  parameters "ds_google_buckets_label_filtered", "param_storage_bucket_list"
  result "result"
  code <<-EOS
  if (param_storage_bucket_list.length > 0) {
    result = _.filter(ds_google_buckets_label_filtered, function(bucket) {
      return _.contains(param_storage_bucket_list, bucket['name']) || _.contains(param_storage_bucket_list, bucket['id'])
    })
  } else {
    result = ds_google_buckets_label_filtered
  }
EOS
end

datasource "ds_google_objects" do
  iterate $ds_google_buckets_name_filtered
  request do
    auth $auth_google
    pagination $pagination_google
    verb "GET"
    host "storage.googleapis.com"
    path join(["/storage/v1/b/", val(iter_item, "name"), "/o"])
    ignore_status [403, 404]
  end
  result do
    encoding "json"
    collect jmes_path(response, "items[*]") do
      field "id", jmes_path(col_item, "id")
      field "kind", jmes_path(col_item, "kind")
      field "metadata", jmes_path(col_item, "metadata")
      field "name", jmes_path(col_item, "name")
      field "contentType", jmes_path(col_item, "contentType")
      field "selfLink", jmes_path(col_item, "selfLink")
      field "size", jmes_path(col_item, "size")
      field "storageClass", jmes_path(col_item, "storageClass")
      field "timeCreated", jmes_path(col_item, "timeCreated")
      field "updated", jmes_path(col_item, "updated")
      field "bucketId", val(iter_item, "id")
      field "bucketName", val(iter_item, "name")
      field "bucketLabels", val(iter_item, "labels")
      field "bucketLocation", val(iter_item, "location")
      field "bucketLocationType", val(iter_item, "locationType")
      field "bucketStorageClass", val(iter_item, "storageClass")
      field "bucketTimeCreated", val(iter_item, "timeCreated")
      field "projectId", val(iter_item, "projectId")
      field "projectName", val(iter_item, "projectName")
      field "projectNumber", val(iter_item, "projectNumber")
    end
  end
end

datasource "ds_google_objects_with_class" do
  run_script $js_google_objects_with_class, $ds_google_objects, $param_nearline_days, $param_coldline_days, $param_new_storage_class
end

script "js_google_objects_with_class", type: "javascript" do
  parameters "ds_google_objects", "param_nearline_days", "param_coldline_days", "param_new_storage_class"
  result "result"
  code <<-EOS
  result = _.map(ds_google_objects, function(object) {
    size_in_gb = Math.round((object['size'] / 1024 / 1024 / 1024) * 100) / 100

    bucketLabels = []

    if (typeof(object['bucketLabels']) == 'object') {
      _.each(Object.keys(object['bucketLabels']), function(key) {
        bucketLabels.push([key, "=", object['bucketLabels'][key]].join(''))
      })
    }

    last_modified_date = new Date(object['updated'])
    creation_time_date = new Date(object['timeCreated'])
    nearline_date = new Date(new Date() - (1000 * 60 * 60 * 24 * param_nearline_days))
    coldline_date = new Date(new Date() - (1000 * 60 * 60 * 24 * param_coldline_days))
    new_storage_class = null

    if (object['storageClass'] != "NEARLINE" && object['storageClass'] != "COLDLINE") {
      if (last_modified_date <= nearline_date && param_new_storage_class != 'COLDLINE') {
        new_storage_class = "NEARLINE"
      }

      if (last_modified_date <= coldline_date && param_new_storage_class != 'NEARLINE') {
        new_storage_class = "COLDLINE"
      }
    }

    return {
      id: object['id'],
      kind: object['kind'],
      metadata: object['metadata'],
      name: object['name'],
      contentType: object['contentType'],
      selfLink: object['selfLink'],
      storageClass: object['storageClass'],
      bucketId: object['bucketId'],
      bucketName: object['bucketName'],
      bucketLocation: object['bucketLocation'],
      bucketLocationType: object['bucketLocationType'],
      bucketStorageClass: object['bucketStorageClass'],
      bucketTimeCreated: object['bucketTimeCreated'],
      bucketLabels: bucketLabels.join(', '),
      projectId: object['projectId'],
      projectName: object['projectName'],
      projectNumber: object['projectNumber'],
      creationTime: creation_time_date.toISOString(),
      lastModified: last_modified_date.toISOString(),
      newStorageClass: new_storage_class,
      size: size_in_gb
    }
  })
EOS
end

datasource "ds_google_objects_incident" do
  run_script $js_google_objects_incident, $ds_google_objects_with_class, $ds_applied_policy, $param_nearline_days, $param_coldline_days, $param_new_storage_class
end

script "js_google_objects_incident", type: "javascript" do
  parameters "ds_google_objects_with_class", "ds_applied_policy", "param_nearline_days", "param_coldline_days", "param_new_storage_class"
  result "result"
  code <<-'EOS'
  objects_to_change = _.reject(ds_google_objects_with_class, function(object) {
    return object['newStorageClass'] == null
  })

  result = _.map(objects_to_change, function(object) {
    recommendationDetails = [
      "Change storage class of Google Object ", object['name'],
      " in Bucket ", object['bucketName'],
      " in Google Project ", object['projectName'], " (", object['projectId'], ")",
      " from ", object['storageClass'], " to ", object['newStorageClass']
    ].join('')

    return {
      accountID: object['projectId'],
      accountName: object['projectName'],
      resourceID: object['name'],
      resourceType: object['storageClass'],
      newResourceType: object['newStorageClass'],
      contentType: object['contentType'],
      tags: object['bucketLabels'],
      id: object['id'],
      kind: object['kind'],
      metadata: object['metadata'],
      selfLink: object['selfLink'],
      size: object['size'],
      bucketId: object['bucketId'],
      bucketName: object['bucketName'],
      bucketLocation: object['bucketLocation'],
      bucketLocationType: object['bucketLocationType'],
      bucketStorageClass: object['bucketStorageClass'],
      bucketTimeCreated: object['bucketTimeCreated'],
      projectNumber: object['projectNumber'],
      creationTime: object['creationTime'],
      lastModified: object['lastModified'],
      policy_name: ds_applied_policy['name'],
      recommendationDetails: recommendationDetails,
      service: "Cloud Storage",
      message: ""
    }
  })

  objects_total = ds_google_objects_with_class.length.toString()
  objects_to_change_total = result.length.toString()
  objects_to_change_percentage = (objects_to_change_total / objects_total * 100).toFixed(2).toString() + '%'

  object_noun = "objects"
  if (objects_total == 1) { object_noun = "object" }

  object_verb = "are"
  if (objects_to_change_total == 1) { object_verb = "is" }

  findings = [
    "Out of ", objects_total, " Google Storage ", object_noun, " analyzed, ",
    objects_to_change_total, " (", objects_to_change_percentage,
    ") ", object_verb, " recommended for a change in storage class. "
  ].join('')

  analysis = ''

  if (param_new_storage_class != 'COLDLINE') {
    nearline_day_noun = "days ago"
    if (param_nearline_days == 1) { nearline_day_noun = "day ago" }

    analysis += [
      "A Google Object is recommended for a change to the 'nearline' storage class ",
      "if it was last modified at least ", param_nearline_days, " ", nearline_day_noun, ". "
    ].join('')
  }

  if (param_new_storage_class != 'NEARLINE') {
    coldline_day_noun = "days ago"
    if (param_coldline_days == 1) { coldline_day_noun = "day ago" }

    analysis += [
      "A Google Object is recommended for a change to the 'coldline' storage class ",
      "if it was last modified at least ", param_coldline_days, " ", coldline_day_noun, "."
    ].join('')
  }

  analysis += "\n\n"

  disclaimer = "The above settings can be modified by editing the applied policy and changing the appropriate parameters."

  // Dummy entry to ensure validation runs at least once
  result.push({ resourceID: "", policy_name: "", message: "", tags: "" })

  result[0]['message'] = findings + analysis + disclaimer
EOS
end

###############################################################################
# Policy
###############################################################################

policy "pol_object_storage_optimization" do
  validate_each $ds_google_objects_incident do
    summary_template "{{ with index data 0 }}{{ .policy_name }}{{ end }}: {{ len data }} Google Storage Objects Recommended For Storage Class Change"
    detail_template "{{ with index data 0 }}{{ .message }}{{ end }}"
    # Policy check fails and incident is created only if data is not empty and the Parent Policy has not been terminated
    check logic_or($ds_parent_policy_terminated, eq(val(item, "resourceID"), ""))
    escalate $esc_email
    escalate $esc_update_objects
    escalate $esc_delete_objects
    hash_exclude "message", "tags"
    export do
      resource_level true
      field "accountID" do
        label "Project ID"
      end
      field "accountName" do
        label "Project Name"
      end
      field "projectNumber" do
        label "Project Number"
      end
      field "bucketName" do
        label "Bucket Name"
      end
      field "resourceID" do
        label "Object Name"
      end
      field "contentType" do
        label "Content Type"
      end
      field "size" do
        label "Size (GiB)"
      end
      field "lastModified" do
        label "Last Modified Date"
      end
      field "recommendationDetails" do
        label "Recommendation"
      end
      field "resourceType" do
        label "Current Storage Class"
      end
      field "newResourceType" do
        label "Recommended Storage Class"
      end
      field "tags" do
        label "Bucket Labels"
      end
      field "id" do
        label "ID"
        path "resourceID"
      end
      field "selfLink" do
        label "Object Link"
      end
    end
  end
end

###############################################################################
# Escalations
###############################################################################

escalation "esc_email" do
  automatic true
  label "Send Email"
  description "Send incident email"
  email $param_email
end

escalation "esc_update_objects" do
  automatic contains($param_automatic_action, "Update Objects Storage Class")
  label "Update Objects Storage Class"
  description "Approval to update the storage class of all selected objects"
  run "update_objects", data
end

escalation "esc_delete_objects" do
  automatic contains($param_automatic_action, "Delete Objects")
  label "Delete Objects"
  description "Approval to delete all selected objects"
  run "delete_objects", data
end

###############################################################################
# Cloud Workflow
###############################################################################

define update_objects($data) return $all_responses do
  $$all_responses = []

  foreach $object in $data do
    sub on_error: handle_error() do
      call update_object($object) retrieve $update_response
      $$all_responses << $update_response
    end
  end

  if inspect($$errors) != "null"
    raise join($$errors, "\n")
  end
end

define delete_objects($data) return $all_responses do
  $$all_responses = []

  foreach $object in $data do
    sub on_error: handle_error() do
      call delete_object($object) retrieve $delete_response
      $$all_responses << $delete_response
    end
  end

  if inspect($$errors) != "null"
    raise join($$errors, "\n")
  end
end

define update_object($object) return $response do
  call url_encode($object['bucketName']) retrieve $bucket_name
  call url_encode($object['resourceID']) retrieve $object_name

  $host = 'storage.googleapis.com'
  $href = join(['/storage/v1/b/', $bucket_name, '/o/', $object_name, '/rewriteTo/b/', $bucket_name, '/o/', $object_name])
  $url = $host + $href
  task_label("POST " + $url)

  $response = http_request({
    auth: $$auth_google,
    verb: 'post',
    host: $host,
    href: $href,
    headers: {
      "cache-control": "no-cache",
      "content-type": "application/json"
    },
    body: {
      "storageClass": $object['newResourceType']
    }
  })

  task_label("Post Google Storage object response: " + $url + " " + to_json($response))
  $$all_responses << to_json({"req": "POST " + $url, "resp": $response})

  if $response["code"] != 200 && $response["code"] != 202 && $response["code"] != 204
    raise "Unexpected response posting Google Storage object: " + $url + " " + to_json($response)
  else
    task_label("Post Google Storage object successful: " + $url)
  end
end

define delete_object($object) return $response do
  call url_encode($object['bucketName']) retrieve $bucket_name
  call url_encode($object['resourceID']) retrieve $object_name

  $host = 'storage.googleapis.com'
  $href = join(['/storage/v1/b/', $bucket_name, '/o/', $object_name])
  $url = $host + $href
  task_label("DELETE " + $url)

  $response = http_request({
    auth: $$auth_google,
    verb: 'delete',
    host: $host,
    href: $href,
    headers: {
      "cache-control": "no-cache",
      "content-type": "application/json"
    }
  })

  task_label("Delete Google Storage object response: " + $url + " " + to_json($response))
  $$all_responses << to_json({"req": "DELETE " + $url, "resp": $response})

  if $response["code"] != 200 && $response["code"] != 202 && $response["code"] != 204
    raise "Unexpected response deleting Google Storage object: " + $url + " " + to_json($response)
  else
    task_label("Delete Google Storage object successful: " + $url)
  end
end

define url_encode($string) return $encoded_string do
  $encoded_string = $string
  $encoded_string = gsub($encoded_string, " ", "%20")
  $encoded_string = gsub($encoded_string, "!", "%21")
  $encoded_string = gsub($encoded_string, "#", "%23")
  $encoded_string = gsub($encoded_string, "$", "%24")
  $encoded_string = gsub($encoded_string, "&", "%26")
  $encoded_string = gsub($encoded_string, "'", "%27")
  $encoded_string = gsub($encoded_string, "(", "%28")
  $encoded_string = gsub($encoded_string, ")", "%29")
  $encoded_string = gsub($encoded_string, "*", "%2A")
  $encoded_string = gsub($encoded_string, "+", "%2B")
  $encoded_string = gsub($encoded_string, ",", "%2C")
  $encoded_string = gsub($encoded_string, "/", "%2F")
  $encoded_string = gsub($encoded_string, ":", "%3A")
  $encoded_string = gsub($encoded_string, ";", "%3B")
  $encoded_string = gsub($encoded_string, "=", "%3D")
  $encoded_string = gsub($encoded_string, "?", "%3F")
  $encoded_string = gsub($encoded_string, "@", "%40")
  $encoded_string = gsub($encoded_string, "[", "%5B")
  $encoded_string = gsub($encoded_string, "]", "%5D")
end

define handle_error() do
  if !$$errors
    $$errors = []
  end
  $$errors << $_error["type"] + ": " + $_error["message"]
  # We check for errors at the end, and raise them all together
  # Skip errors handled by this definition
  $_error_behavior = "skip"
end

###############################################################################
# Meta Policy [alpha]
# Not intended to be modified or used by policy developers
###############################################################################

# If the meta_parent_policy_id is not set it will evaluate to an empty string and we will look for the policy itself,
# if it is set we will look for the parent policy.
datasource "ds_get_policy" do
  request do
    auth $auth_flexera
    host rs_governance_host
    ignore_status [404]
    path join(["/api/governance/projects/", rs_project_id, "/applied_policies/", switch(ne(meta_parent_policy_id, ""), meta_parent_policy_id, policy_id)])
    header "Api-Version", "1.0"
  end
  result do
    encoding "json"
    field "id", jmes_path(response, "id")
  end
end

datasource "ds_parent_policy_terminated" do
  run_script $js_decide_if_self_terminate, $ds_get_policy, policy_id, meta_parent_policy_id
end

# If the policy was applied by a meta_parent_policy we confirm it exists if it doesn't we confirm we are deleting
# This information is used in two places:
# - determining whether or not we make a delete call
# - determining if we should create an incident (we don't want to create an incident on the run where we terminate)
script "js_decide_if_self_terminate", type: "javascript" do
  parameters "found", "self_policy_id", "meta_parent_policy_id"
  result "result"
  code <<-EOS
  var result
  if (meta_parent_policy_id != "" && found.id == undefined) {
    result = true
  } else {
    result = false
  }
  EOS
end

# Two potentials ways to set this up:
# - this way and make a unneeded 'get' request when not deleting
# - make the delete request an interate and have it iterate over an empty array when not deleting and an array with one item when deleting
script "js_make_terminate_request", type: "javascript" do
  parameters "should_delete", "policy_id", "rs_project_id", "rs_governance_host"
  result "request"
  code <<-EOS

  var request = {
    auth:  'auth_flexera',
    host: rs_governance_host,
    path: "/api/governance/projects/" + rs_project_id + "/applied_policies/" + policy_id,
    headers: {
      "API-Version": "1.0",
      "Content-Type":"application/json"
    },
  }

  if (should_delete) {
    request.verb = 'DELETE'
  }
  EOS
end

datasource "ds_terminate_self" do
  request do
    run_script $js_make_terminate_request, $ds_parent_policy_terminated, policy_id, rs_project_id, rs_governance_host
  end
end

datasource "ds_is_deleted" do
  run_script $js_check_deleted, $ds_terminate_self
end

# This is just a way to have the check delete request connect to the farthest leaf from policy.
# We want the delete check to the first thing the policy does to avoid the policy erroring before it can decide whether or not it needs to self terminate
# Example a customer deletes a credential and then terminates the parent policy. We still want the children to self terminate
# The only way I could see this not happening is if the user who applied the parent_meta_policy was offboarded or lost policy access, the policies who are impersonating the user
# would not have access to self-terminate
# It may be useful for the backend to enable a mass terminate at some point for all meta_child_policies associated with an id.
script "js_check_deleted", type: "javascript" do
  parameters "response"
  result "result"
  code <<-EOS
  result = {"path":"/"}
  EOS
end
